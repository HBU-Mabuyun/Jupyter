{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本节课目标：搭建神经网络，总结搭建八股"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 Tensorflow 的 NN ：\n",
    "用张量表示数据，计算图搭建神经网络，会话执行计算图，优化线上的权重，得到模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量：多维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([1.0,2.0])\n",
    "b = tf.constant([2.0,3.0])\n",
    "result = a + b\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图：搭建神经网络的计算过程，是承载一个或多个计算节点的一张图，只搭建网络，不运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_3:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0,2.0]])\n",
    "w = tf.constant([[3.0],[4.0]])\n",
    "y = tf.matmul(x,w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print (sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 准备数据集，提取特征，作为输入喂给NN\n",
    "2. 搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）\n",
    "3. 大量特征数据喂给NN，迭代优化NN参数\n",
    "4. 使用训练好的模型预测和分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(y,feed_dict={x:[[0.7,0.5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.0904665 ]\n",
      " [ 1.2236414 ]\n",
      " [ 1.72707319]\n",
      " [ 2.23050475]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(y,feed_dict={x:[[0.7,0.5],[0.2,0.3],[0.3,0.4],[0.4,0.5]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数loss\n",
    "梯度下降 momentum优化 adam优化\n",
    "均方误差MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子(TF八股文，老师要求背下来)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #0导入模块，生成模拟数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[ 0.83494319  0.11482951]\n",
      " [ 0.66899751  0.46594987]\n",
      " [ 0.60181666  0.58838408]\n",
      " [ 0.31836656  0.20502072]\n",
      " [ 0.87043944  0.02679395]\n",
      " [ 0.41539811  0.43938369]\n",
      " [ 0.68635684  0.24833404]\n",
      " [ 0.97315228  0.68541849]\n",
      " [ 0.03081617  0.89479913]\n",
      " [ 0.24665715  0.28584862]\n",
      " [ 0.31375667  0.47718349]\n",
      " [ 0.56689254  0.77079148]\n",
      " [ 0.7321604   0.35828963]\n",
      " [ 0.15724842  0.94294584]\n",
      " [ 0.34933722  0.84634483]\n",
      " [ 0.50304053  0.81299619]\n",
      " [ 0.23869886  0.9895604 ]\n",
      " [ 0.4636501   0.32531094]\n",
      " [ 0.36510487  0.97365522]\n",
      " [ 0.73350238  0.83833013]\n",
      " [ 0.61810158  0.12580353]\n",
      " [ 0.59274817  0.18779828]\n",
      " [ 0.87150299  0.34679501]\n",
      " [ 0.25883219  0.50002932]\n",
      " [ 0.75690948  0.83429824]\n",
      " [ 0.29316649  0.05646578]\n",
      " [ 0.10409134  0.88235166]\n",
      " [ 0.06727785  0.57784761]\n",
      " [ 0.38492705  0.48384792]\n",
      " [ 0.69234428  0.19687348]\n",
      " [ 0.42783492  0.73416985]\n",
      " [ 0.09696069  0.04883936]]\n",
      "Y_:\n",
      " [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "#基于seed产生随机数\n",
    "rdm = np.random.RandomState(SEED)\n",
    "#随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集\n",
    "X = rdm.rand(32,2)\n",
    "#从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如果和不小于1 给Y赋值0 \n",
    "#作为输入数据集的标签（正确答案） \n",
    "Y_ = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "print (\"X:\\n\",X)\n",
    "print (\"Y_:\\n\",Y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1定义神经网络的输入、参数和输出,定义前向传播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_= tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1= tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2= tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2定义损失函数及反向传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = tf.reduce_mean(tf.square(y-y_)) \n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss_mse)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3生成会话，训练STEPS轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2:\n",
      " [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s), loss_mse on all data is 5.13118\n",
      "After 500 training step(s), loss_mse on all data is 0.429111\n",
      "After 1000 training step(s), loss_mse on all data is 0.409789\n",
      "After 1500 training step(s), loss_mse on all data is 0.399923\n",
      "After 2000 training step(s), loss_mse on all data is 0.394146\n",
      "After 2500 training step(s), loss_mse on all data is 0.390597\n",
      "After 3000 training step(s), loss_mse on all data is 0.388336\n",
      "After 3500 training step(s), loss_mse on all data is 0.386855\n",
      "After 4000 training step(s), loss_mse on all data is 0.385863\n",
      "After 4500 training step(s), loss_mse on all data is 0.385187\n",
      "After 5000 training step(s), loss_mse on all data is 0.384719\n",
      "After 5500 training step(s), loss_mse on all data is 0.384391\n",
      "After 6000 training step(s), loss_mse on all data is 0.38416\n",
      "After 6500 training step(s), loss_mse on all data is 0.383995\n",
      "After 7000 training step(s), loss_mse on all data is 0.383877\n",
      "After 7500 training step(s), loss_mse on all data is 0.383791\n",
      "After 8000 training step(s), loss_mse on all data is 0.383729\n",
      "After 8500 training step(s), loss_mse on all data is 0.383684\n",
      "After 9000 training step(s), loss_mse on all data is 0.383652\n",
      "After 9500 training step(s), loss_mse on all data is 0.383628\n",
      "\n",
      "\n",
      "w1:\n",
      " [[-0.69167352  0.81592691  0.09629341]\n",
      " [-2.34338474 -0.10742698  0.58547068]]\n",
      "w2:\n",
      " [[-0.08710446]\n",
      " [ 0.81644469]\n",
      " [-0.05229824]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # 输出目前（未经训练）的参数取值。\n",
    "    print (\"w1:\\n\", sess.run(w1))\n",
    "    print (\"w2:\\n\", sess.run(w2))\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    # 训练模型。\n",
    "    STEPS = 10000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss_mse, feed_dict={x: X, y_: Y_})\n",
    "            print(\"After %d training step(s), loss_mse on all data is %g\" % (i, total_loss))\n",
    "    \n",
    "    # 输出训练后的参数取值。\n",
    "    print (\"\\n\")\n",
    "    print( \"w1:\\n\", sess.run(w1))\n",
    "    print (\"w2:\\n\", sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.496045683\n",
      "1 0.483812182\n",
      "2 0.471328371\n",
      "3 0.458620047\n",
      "4 0.445716575\n",
      "5 0.432650702\n",
      "6 0.41945825\n",
      "7 0.406177712\n",
      "8 0.392849727\n",
      "9 0.379516479\n",
      "10 0.366221008\n",
      "11 0.353006485\n",
      "12 0.339915464\n",
      "13 0.326989145\n",
      "14 0.314266677\n",
      "15 0.301784539\n",
      "16 0.289576008\n",
      "17 0.277670734\n",
      "18 0.266094445\n",
      "19 0.254868764\n",
      "20 0.244011153\n",
      "21 0.233534958\n",
      "22 0.223449561\n",
      "23 0.213760599\n",
      "24 0.204470255\n",
      "25 0.195577581\n",
      "26 0.18707886\n",
      "27 0.178967972\n",
      "28 0.171236756\n",
      "29 0.163875366\n",
      "30 0.156872606\n",
      "31 0.150216233\n",
      "32 0.143893239\n",
      "33 0.137890098\n",
      "34 0.13219298\n",
      "35 0.126787937\n",
      "36 0.12166106\n",
      "37 0.116798604\n",
      "38 0.112187099\n",
      "39 0.107813425\n",
      "40 0.103664883\n",
      "41 0.099729235\n",
      "42 0.095994742\n",
      "43 0.092450179\n",
      "44 0.089084853\n",
      "45 0.085888597\n",
      "46 0.082851774\n",
      "47 0.079965262\n",
      "48 0.077220445\n",
      "49 0.074609194\n",
      "50 0.072123848\n",
      "51 0.0697572\n",
      "52 0.067502469\n",
      "53 0.065353283\n",
      "54 0.063303658\n",
      "55 0.061347974\n",
      "56 0.059480957\n",
      "57 0.057697657\n",
      "58 0.055993433\n",
      "59 0.054363927\n",
      "60 0.052805052\n",
      "61 0.051312973\n",
      "62 0.049884092\n",
      "63 0.048515033\n",
      "64 0.047202624\n",
      "65 0.045943889\n",
      "66 0.044736033\n",
      "67 0.043576428\n",
      "68 0.042462606\n",
      "69 0.041392245\n",
      "70 0.040363165\n",
      "71 0.039373309\n",
      "72 0.038420746\n",
      "73 0.037503654\n",
      "74 0.036620319\n",
      "75 0.035769124\n",
      "76 0.034948546\n",
      "77 0.034157146\n",
      "78 0.033393567\n",
      "79 0.032656529\n",
      "80 0.031944821\n",
      "81 0.031257299\n",
      "82 0.030592881\n",
      "83 0.029950545\n",
      "84 0.029329323\n",
      "85 0.028728296\n",
      "86 0.028146598\n",
      "87 0.027583404\n",
      "88 0.027037934\n",
      "89 0.026509449\n",
      "90 0.025997244\n",
      "91 0.025500653\n",
      "92 0.025019043\n",
      "93 0.024551811\n",
      "94 0.024098385\n",
      "95 0.023658219\n",
      "96 0.023230796\n",
      "97 0.022815623\n",
      "98 0.022412229\n",
      "99 0.022020166\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "import random\n",
    "import math\n",
    "\n",
    "#\n",
    "#   参数解释：\n",
    "#   \"pd_\" ：偏导的前缀\n",
    "#   \"d_\" ：导数的前缀\n",
    "#   \"w_ho\" ：隐含层到输出层的权重系数索引\n",
    "#   \"w_ih\" ：输入层到隐含层的权重系数的索引\n",
    "\n",
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "\n",
    "        # 1. 输出神经元的值\n",
    "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "\n",
    "            # ∂E/∂zⱼ\n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
    "\n",
    "        # 2. 隐含层神经元的值\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "\n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "            d_error_wrt_hidden_neuron_output = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "\n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
    "\n",
    "        # 3. 更新输出层权重系数\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "        # 4. 更新隐含层的权重系数\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
    "\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "\n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
    "        return total_error\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, bias):\n",
    "\n",
    "        # 同一层的神经元共享一个截距项b\n",
    "        self.bias = bias if bias else random.random()\n",
    "\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.squash(self.calculate_total_net_input())\n",
    "        return self.output\n",
    "\n",
    "    def calculate_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    # 激活函数sigmoid\n",
    "    def squash(self, total_net_input):\n",
    "        return 1 / (1 + math.exp(-total_net_input))\n",
    "\n",
    "\n",
    "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
    "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
    "\n",
    "    # 每一个神经元的误差是由平方差公式计算的\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5 * (target_output - self.output) ** 2\n",
    "\n",
    "    \n",
    "    def calculate_pd_error_wrt_output(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "\n",
    "    \n",
    "    def calculate_pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "\n",
    "\n",
    "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
    "        return self.inputs[index]\n",
    "\n",
    "\n",
    "# 文中的例子:\n",
    "\n",
    "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
    "for i in range(100):\n",
    "    nn.train([0.05, 0.1], [0.01, 0.09])\n",
    "    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.09]]]), 9))\n",
    "\n",
    "\n",
    "#另外一个例子，可以把上面的例子注释掉再运行一下:\n",
    "\n",
    "# training_sets = [\n",
    "#     [[0, 0], [0]],\n",
    "#     [[0, 1], [1]],\n",
    "#     [[1, 0], [1]],\n",
    "#     [[1, 1], [0]]\n",
    "# ]\n",
    "\n",
    "# nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1]))\n",
    "# for i in range(10000):\n",
    "#     training_inputs, training_outputs = random.choice(training_sets)\n",
    "#     nn.train(training_inputs, training_outputs)\n",
    "#     print(i, nn.calculate_total_error(training_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
