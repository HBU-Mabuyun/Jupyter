{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using tf.estimator API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf  # pylint: disable=g-bad-import-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'official'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-386fe257e839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhooks_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\GitHub\\Jupyter\\TensorFlowDevelop\\hooks_helper.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'official'"
     ]
    }
   ],
   "source": [
    "import parsers\n",
    "import hooks_helper\n",
    "import model_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\n",
    "LOSS_PREFIX = {'wide': 'linear/', 'deep': 'dnn/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_columns():\n",
    "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
    "  # Continuous columns\n",
    "  age = tf.feature_column.numeric_column('age')\n",
    "  education_num = tf.feature_column.numeric_column('education_num')\n",
    "  capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "  capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "  hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "  education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'education', [\n",
    "          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "  marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'marital_status', [\n",
    "          'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "          'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "  relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'relationship', [\n",
    "          'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "          'Other-relative'])\n",
    "\n",
    "  workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'workclass', [\n",
    "          'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "          'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "  # To show an example of hashing:\n",
    "  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      'occupation', hash_bucket_size=1000)\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.feature_column.bucketized_column(\n",
    "      age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  base_columns = [\n",
    "      education, marital_status, relationship, workclass, occupation,\n",
    "      age_buckets,\n",
    "  ]\n",
    "\n",
    "  crossed_columns = [\n",
    "      tf.feature_column.crossed_column(\n",
    "          ['education', 'occupation'], hash_bucket_size=1000),\n",
    "      tf.feature_column.crossed_column(\n",
    "          [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
    "  ]\n",
    "\n",
    "  wide_columns = base_columns + crossed_columns\n",
    "\n",
    "  deep_columns = [\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "      tf.feature_column.indicator_column(workclass),\n",
    "      tf.feature_column.indicator_column(education),\n",
    "      tf.feature_column.indicator_column(marital_status),\n",
    "      tf.feature_column.indicator_column(relationship),\n",
    "      # To show an example of embedding\n",
    "      tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "  ]\n",
    "\n",
    "  return wide_columns, deep_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "  wide_columns, deep_columns = build_model_columns()\n",
    "  hidden_units = [100, 75, 50, 25]\n",
    "\n",
    "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "  # trains faster than GPU for this model.\n",
    "  run_config = tf.estimator.RunConfig().replace(\n",
    "      session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "  if model_type == 'wide':\n",
    "    return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=wide_columns,\n",
    "        config=run_config)\n",
    "  elif model_type == 'deep':\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "  else:\n",
    "    return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have run data_download.py and '\n",
    "      'set the --data_dir argument to the correct path.' % data_file)\n",
    "\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('income_bracket')\n",
    "    return features, tf.equal(labels, '>50K')\n",
    "\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
    "\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "  parser = WideDeepArgParser()\n",
    "  flags = parser.parse_args(args=argv[1:])\n",
    "\n",
    "  # Clean up the model directory if present\n",
    "  shutil.rmtree(flags.model_dir, ignore_errors=True)\n",
    "  model = build_estimator(flags.model_dir, flags.model_type)\n",
    "\n",
    "  train_file = os.path.join(flags.data_dir, 'adult.data')\n",
    "  test_file = os.path.join(flags.data_dir, 'adult.test')\n",
    "\n",
    "  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "  def train_input_fn():\n",
    "    return input_fn(\n",
    "        train_file, flags.epochs_between_evals, True, flags.batch_size)\n",
    "\n",
    "  def eval_input_fn():\n",
    "    return input_fn(test_file, 1, False, flags.batch_size)\n",
    "\n",
    "  loss_prefix = LOSS_PREFIX.get(flags.model_type, '')\n",
    "  train_hooks = hooks_helper.get_train_hooks(\n",
    "      flags.hooks, batch_size=flags.batch_size,\n",
    "      tensors_to_log={'average_loss': loss_prefix + 'head/truediv',\n",
    "                      'loss': loss_prefix + 'head/weighted_loss/Sum'})\n",
    "\n",
    "  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "  for n in range(flags.train_epochs // flags.epochs_between_evals):\n",
    "    model.train(input_fn=train_input_fn, hooks=train_hooks)\n",
    "    results = model.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * flags.epochs_between_evals)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))\n",
    "\n",
    "    if model_helpers.past_stop_threshold(\n",
    "        flags.stop_threshold, results['accuracy']):\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepArgParser(argparse.ArgumentParser):\n",
    "  \"\"\"Argument parser for running the wide deep model.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super(WideDeepArgParser, self).__init__(parents=[parsers.BaseParser()])\n",
    "    self.add_argument(\n",
    "        '--model_type', '-mt', type=str, default='wide_deep',\n",
    "        choices=['wide', 'deep', 'wide_deep'],\n",
    "        help='[default %(default)s] Valid model types: wide, deep, wide_deep.',\n",
    "        metavar='<MT>')\n",
    "    self.set_defaults(\n",
    "        data_dir='/tmp/census_data',\n",
    "        model_dir='/tmp/census_model',\n",
    "        train_epochs=40,\n",
    "        epochs_between_evals=2,\n",
    "        batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  main(argv=sys.argv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
