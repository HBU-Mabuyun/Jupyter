{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF的eager execution是一个命令式编程环境，可以立即执行操作，不需要建计算图（graph）。\n",
    "\n",
    "操作返回具体的值，而不是构建一个计算图以便以后运行。\n",
    "\n",
    "这使得初学TensorFlow、调试模型、减少模板代码变得很容易，而且很有趣！\n",
    "\n",
    "Eager execution支持大多数的TensorFlow操作和GPU加速。\n",
    "\n",
    "自动微分使用动态构建的tape代替静态图来计算梯度。\n",
    "\n",
    "Eager execution是一个灵活的机器学习平台，用于研究和实验，提供：\n",
    "\n",
    "- 直观的界面：自然地构造代码并使用Python数据结构。快速迭代小模型和小数据。\n",
    "- 更容易调试：直接调用ops来检查运行的模型和测试更改。使用标准的Python调试工具进行即时错误报告。\n",
    "- 自然的控制流：使用Python控制流而不是图控制流，包括对动态模型的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装和基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 更新到TF 1.7版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[ 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))  # => \"hello, [[4.]]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager execution works nicely with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n",
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],[3, 4]])\n",
    "print(a)\n",
    "# => tf.Tensor([[1 2][3 4]], shape=(2, 2), dtype=int32)\n",
    "\n",
    "# Broadcasting support\n",
    "b = tf.add(a, 1)\n",
    "print(b)\n",
    "# => tf.Tensor([[2 3][4 5]], shape=(2, 2), dtype=int32)\n",
    "\n",
    "# Operator overloading is supported\n",
    "print(a * b)\n",
    "# => tf.Tensor([[ 2  6][12 20]], shape=(2, 2), dtype=int32)\n",
    "\n",
    "# Use NumPy values\n",
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "# => [[ 2  6]\n",
    "#     [12 20]]\n",
    "\n",
    "# Obtain numpy value from a tensor:\n",
    "print(a.numpy())\n",
    "# => [[1 2]\n",
    "#     [3 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=31, shape=(1, 1), dtype=float32, numpy=array([[ 2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w = tfe.Variable([[1.0]])\n",
    "with tfe.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad)  # => [tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "records forward-pass operations to train a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 67.637\n",
      "Loss at step 000: 65.037\n",
      "Loss at step 020: 29.902\n",
      "Loss at step 040: 14.052\n",
      "Loss at step 060: 6.896\n",
      "Loss at step 080: 3.663\n",
      "Loss at step 100: 2.200\n",
      "Loss at step 120: 1.538\n",
      "Loss at step 140: 1.238\n",
      "Loss at step 160: 1.102\n",
      "Loss at step 180: 1.040\n",
      "Final loss: 1.013\n",
      "W = 3.0690221786499023, B = 2.159123420715332\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "  return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(weights, biases) \n",
    "  return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "  dW, dB = grad(W, B)\n",
    "  W.assign_sub(dW * learning_rate)\n",
    "  B.assign_sub(dB * learning_rate)\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data.train.images,\n",
    "                                              data.train.labels))\n",
    "...\n",
    "for (batch, (images, labels)) in enumerate(tfe.Iterator(dataset)):\n",
    "  ...\n",
    "  with tfe.GradientTape() as tape:\n",
    "    logits = model(images, training=True)\n",
    "    loss_value = loss(logits, labels)\n",
    "  ...\n",
    "  grads = tape.gradient(loss_value, model.variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    # Variables are automatically recorded, but manually watch a tensor\n",
    "    tape.watch(init_x)\n",
    "    value = fn(init_x)\n",
    "  grad, = tape.gradient(value, [init_x])\n",
    "  grad_norm = tf.reduce_sum(grad * grad)\n",
    "  init_value = value\n",
    "  while value > init_value - rate * grad_norm:\n",
    "    x = init_x - rate * grad\n",
    "    value = fn(x)\n",
    "    rate /= 2.0\n",
    "  return x, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=7421, shape=(), dtype=float32, numpy=-1.0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "  return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "square(3.)  # => 9.0\n",
    "grad(3.)    # => [6.0]\n",
    "\n",
    "# The second-order derivative of square:\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "gradgrad(3.)  # => [2.0]\n",
    "\n",
    "# The third-order derivative is None:\n",
    "gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x)[0])\n",
    "gradgradgrad(3.)  # => [None]\n",
    "\n",
    "# With flow control:\n",
    "def abs(x):\n",
    "  return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "grad(3.)   # => [1.0]\n",
    "grad(-3.)  # => [-1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "  y = tf.identity(x)\n",
    "  def grad_fn(dresult):\n",
    "    return [tf.clip_by_norm(dresult, norm), None]\n",
    "  return y, grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=7438, shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "  return tf.log(1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "grad_log1pexp(0.)  # => [0.5]\n",
    "\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "grad_log1pexp(100.)  # => [nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=7457, shape=(), dtype=float32, numpy=1.0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(0.)  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "grad_log1pexp(100.)  # => [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "    self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    result = self.dense1(input)\n",
    "    result = self.dense2(result)\n",
    "    result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "    return result\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MNISTModel at 0x23cb3204550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing a blank image\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "result = model(batch)\n",
    "# => tf.Tensor([[[ 0.  0., ..., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7591, shape=(1, 1, 10), dtype=float32, numpy=array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset  # download dataset.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.460\n",
      "Loss at step 0000: 2.551\n",
      "Loss at step 0200: 2.240\n",
      "Loss at step 0400: 2.206\n",
      "Loss at step 0600: 2.025\n",
      "Loss at step 0800: 2.078\n",
      "Loss at step 1000: 1.738\n",
      "Loss at step 1200: 1.619\n",
      "Loss at step 1400: 1.755\n",
      "Loss at step 1600: 1.406\n",
      "Loss at step 1800: 1.609\n",
      "Loss at step 2000: 1.235\n",
      "Loss at step 2200: 1.347\n",
      "Loss at step 2400: 1.210\n",
      "Loss at step 2600: 1.258\n",
      "Loss at step 2800: 1.056\n",
      "Loss at step 3000: 1.285\n",
      "Loss at step 3200: 1.256\n",
      "Loss at step 3400: 0.793\n",
      "Loss at step 3600: 0.929\n",
      "Loss at step 3800: 1.054\n",
      "Loss at step 4000: 0.839\n",
      "Loss at step 4200: 0.742\n",
      "Loss at step 4400: 0.909\n",
      "Loss at step 4600: 0.752\n",
      "Loss at step 4800: 1.248\n",
      "Loss at step 5000: 1.021\n",
      "Loss at step 5200: 0.955\n",
      "Loss at step 5400: 0.802\n",
      "Loss at step 5600: 0.506\n",
      "Loss at step 5800: 0.512\n",
      "Loss at step 6000: 0.787\n",
      "Loss at step 6200: 0.584\n",
      "Loss at step 6400: 0.582\n",
      "Loss at step 6600: 0.788\n",
      "Loss at step 6800: 0.578\n",
      "Loss at step 7000: 0.741\n",
      "Loss at step 7200: 0.740\n",
      "Loss at step 7400: 0.799\n",
      "Final loss: 0.947\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = tfe.Iterator(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(tfe.Iterator(dataset_train)):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  for (i, (x, y)) in enumerate(tfe.Iterator(dataset_train)):\n",
    "    # minimize() is equivalent to the grad() and apply_gradients() calls.\n",
    "    optimizer.minimize(lambda: loss(model, x, y),\n",
    "                       global_step=tf.train.get_or_create_global_step())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 68.647\n",
      "Loss at step 000: 65.951\n",
      "Loss at step 020: 29.793\n",
      "Loss at step 040: 13.766\n",
      "Loss at step 060: 6.660\n",
      "Loss at step 080: 3.510\n",
      "Loss at step 100: 2.114\n",
      "Loss at step 120: 1.494\n",
      "Loss at step 140: 1.219\n",
      "Loss at step 160: 1.098\n",
      "Loss at step 180: 1.044\n",
      "Loss at step 200: 1.020\n",
      "Loss at step 220: 1.009\n",
      "Loss at step 240: 1.004\n",
      "Loss at step 260: 1.002\n",
      "Loss at step 280: 1.001\n",
      "Final loss: 1.001\n",
      "W = 3.0005478858947754, B = 2.0684731006622314\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tfe.Variable(5., name='weight')\n",
    "    self.B = tfe.Variable(10., name='bias')\n",
    "  def predict(self, inputs):\n",
    "    return inputs * self.W + self.B\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model.predict(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tfe.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use objects for state during eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=672106, shape=(), dtype=float64, numpy=5.5>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tfe.metrics.Mean(\"loss\")\n",
    "m(0)\n",
    "m(5)\n",
    "m.result()  # => 2.5\n",
    "m([8, 9])\n",
    "m.result()  # => 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to multiply a (10000, 10000) matrix by itself 2000 times:\n",
      "CPU: 1.1401684284210205 secs\n",
      "GPU: 1.2664141654968262 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "def measure(x, steps):\n",
    "  # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "  tf.matmul(x, x)\n",
    "  start = time.time()\n",
    "  for i in range(steps):\n",
    "    x = tf.matmul(x, x)\n",
    "#     _ = x.numpy()  # Make sure to execute op and not just enqueue it\n",
    "  end = time.time()\n",
    "  return end - start\n",
    "\n",
    "shape = (10000, 10000)\n",
    "steps = 2000\n",
    "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    " print(\"CPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "\n",
    "# Run on GPU, if available:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "else:\n",
    "  print(\"GPU: not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use eager execution in a graph environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def my_py_func(x):\n",
    "  x = tf.matmul(x, x)  # You can use tf ops\n",
    "  print(x)  # but it's eager!\n",
    "  return x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  x = tf.placeholder(dtype=tf.float32)\n",
    "  # Call eager function in graph!\n",
    "  pf = tfe.py_func(my_py_func, [x], tf.float32)\n",
    "  sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
