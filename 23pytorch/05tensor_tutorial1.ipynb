{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in Torch.\n",
    "\n",
    "张量在PyTorch上的表现与在Torch上几乎是一样的。\n",
    "\n",
    "Create a tensor of size (5 x 7) with uninitialized memory:\n",
    "\n",
    "\n",
    "用未初始化的内存创建一个大小（5 x 7）的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.empty(5, 7, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3266e-39,  7.3470e-39,  1.0194e-38,  1.0469e-38,  1.0010e-38,\n",
       "          6.4285e-39,  9.9184e-39],\n",
       "        [ 1.0561e-38,  7.1633e-39,  6.7041e-39,  6.7041e-39,  2.9388e-39,\n",
       "          1.0194e-38,  1.0286e-38],\n",
       "        [ 1.0469e-38,  1.0653e-38,  1.0194e-38,  8.4490e-39,  1.0837e-38,\n",
       "          1.0194e-38,  9.9184e-39],\n",
       "        [ 6.3368e-39,  1.0653e-38,  9.0000e-39,  1.0102e-38,  1.1020e-38,\n",
       "          4.7755e-39,  9.9184e-39],\n",
       "        [ 9.0000e-39,  1.0010e-38,  9.9184e-39,  1.0194e-38,  4.2246e-39,\n",
       "          9.9184e-39,  1.5134e-43]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a double tensor randomized with a normal distribution with mean=0,var=1:\n",
    "\n",
    "初始化一个具有正态分布的double类型张量，均值=0，var=1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2750, -1.0954, -0.3850, -0.4567,  0.1513,  1.2243,  0.2648],\n",
      "        [ 0.1127, -1.7398, -1.3294,  0.3334,  1.0473,  0.8184, -1.5414],\n",
      "        [-0.9335, -0.5500, -1.3084, -1.7224, -0.4345,  0.4474,  0.3734],\n",
      "        [-1.7688,  0.3545, -0.0823,  1.5180,  0.1939,  1.0314,  0.8944],\n",
      "        [ 0.5676, -0.5146,  2.1145,  1.5334, -1.4174,  0.3882, -1.9060]], dtype=torch.float64)\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n",
    "``torch.Size``实际上是一个元组，所以它支持相同的操作\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate in-place on it will have an ``_`` postfix. \n",
    "\n",
    "第一个区别是，在它上面的张量上的所有操作都有一个“后缀”。\n",
    "\n",
    "For example, ``add`` is the out-of-place version, and ``add_`` is the in-place version.\n",
    "\n",
    "例如，add是 out-of-place版本，add_ 是in-place版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
       "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
       "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
       "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
       "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like ``narrow`` do not have in-place versions, and hence, ``.narrow_`` does not exist.\n",
    "\n",
    "一些像``narrow``这样的操作没有in-place版本，因此，``.narrow_``并不存在。\n",
    "\n",
    "Similarly, some operations like ``fill_`` do not have an out-of-place version, so ``.fill`` does not exist.\n",
    "\n",
    "类似地，``fill_``之类的操作没有一个out-of-place版本，所以``.fill``不存在。\n",
    "\n",
    "Zero Indexing 零索引\n",
    "-------------\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (In lua, tensors are one-indexed)\n",
    "\n",
    "另一个区别是，张量是零索引的。（在lua中，张量是一个索引的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing\n",
    "\n",
    "张量也可以用Python的切片进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000],\n",
       "        [ 3.5000,  3.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing 不再使用驼峰命名法\n",
    "---------------\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase anymore. \n",
    "\n",
    "下一个小的区别是，所有的函数现在都不再是camelCase了。(camelCase叫做“骆驼式命名法”，是指在英语中，依靠单词的大小写拼写复合词的做法。)\n",
    "\n",
    "For example ``indexAdd`` is now called ``index_add_``\n",
    "\n",
    "例如``indexAdd`` 现在写作 ``index_add_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.],\n",
      "        [ 101.,    1.,    1.,    1.,   11.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.tensor([4, 0], dtype=torch.long), z)# z 的 第二列 与 x 的 第一列 相加， z的 第一列 与 x 的第五列 相加\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 201.,    1.,   11.,  101.,   21.],\n",
      "        [ 201.,    1.,   11.,  101.,   21.],\n",
      "        [ 201.,    1.,   11.,  101.,   21.],\n",
      "        [ 201.,    1.,   11.,  101.,   21.],\n",
      "        [ 201.,    1.,   11.,  101.,   21.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.tensor([2, 3], dtype=torch.long), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "\n",
    "将一个torch张量转换成一个numpy阵列，反之亦然。\n",
    "\n",
    "The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.\n",
    "\n",
    "torch张量和numpy阵列将共享它们的底层内存位置，而改变一个则会改变另一个。\n",
    "\n",
    "Converting torch Tensor to numpy Array\n",
    "\n",
    "将torch张量转换为numpy阵列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  2.,  2.,  2.,  2.])\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)  # see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy Array to torch Tensor\n",
    "\n",
    "将numpy数组转换成火炬张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back.\n",
    "\n",
    "CPU上所有的张量（除了CharTensor）支持转换为NumPy和转回。\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and transfering a CUDA tensor from the CPU to GPU will retain its underlying type.\n",
    "\n",
    "CUDA张量在pytorch中很好也很容易，将一个CUDA张量从CPU转移到GPU上，将保留它的底层类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # creates a LongTensor and transfers it to GPU as torch.cuda.LongTensor\n",
    "    a = torch.full((10,), 3, device=torch.device(\"cuda\"))\n",
    "    print(type(a))\n",
    "    b = a.to(torch.device(\"cpu\"))\n",
    "    # transfers it to CPU, back to being a torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
