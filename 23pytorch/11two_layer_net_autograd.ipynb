{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
    "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
    "holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32798656.0\n",
      "1 33781912.0\n",
      "2 40981376.0\n",
      "3 45741272.0\n",
      "4 39798520.0\n",
      "5 24309458.0\n",
      "6 10909502.0\n",
      "7 4398286.5\n",
      "8 2077015.125\n",
      "9 1269201.625\n",
      "10 933312.125\n",
      "11 749510.8125\n",
      "12 624568.875\n",
      "13 529389.3125\n",
      "14 453148.9375\n",
      "15 390422.5625\n",
      "16 338267.46875\n",
      "17 294451.84375\n",
      "18 257414.8125\n",
      "19 225881.1875\n",
      "20 198895.390625\n",
      "21 175652.140625\n",
      "22 155607.765625\n",
      "23 138231.5625\n",
      "24 123107.15625\n",
      "25 109926.2265625\n",
      "26 98393.796875\n",
      "27 88251.0546875\n",
      "28 79323.53125\n",
      "29 71448.109375\n",
      "30 64473.734375\n",
      "31 58282.625\n",
      "32 52772.95703125\n",
      "33 47856.37109375\n",
      "34 43462.1796875\n",
      "35 39525.546875\n",
      "36 35996.30859375\n",
      "37 32821.109375\n",
      "38 29966.099609375\n",
      "39 27393.0234375\n",
      "40 25070.84765625\n",
      "41 22969.890625\n",
      "42 21067.201171875\n",
      "43 19340.84375\n",
      "44 17772.013671875\n",
      "45 16345.4873046875\n",
      "46 15052.5751953125\n",
      "47 13876.5888671875\n",
      "48 12802.853515625\n",
      "49 11821.9130859375\n",
      "50 10924.7431640625\n",
      "51 10102.578125\n",
      "52 9348.9599609375\n",
      "53 8657.5029296875\n",
      "54 8022.34765625\n",
      "55 7438.65087890625\n",
      "56 6901.81494140625\n",
      "57 6408.32421875\n",
      "58 5953.98583984375\n",
      "59 5534.53466796875\n",
      "60 5147.572265625\n",
      "61 4790.10546875\n",
      "62 4459.767578125\n",
      "63 4154.6103515625\n",
      "64 3872.05322265625\n",
      "65 3610.37890625\n",
      "66 3368.052978515625\n",
      "67 3143.34423828125\n",
      "68 2935.02880859375\n",
      "69 2741.660400390625\n",
      "70 2562.0009765625\n",
      "71 2395.016357421875\n",
      "72 2239.71240234375\n",
      "73 2095.317626953125\n",
      "74 1960.926025390625\n",
      "75 1835.8212890625\n",
      "76 1719.2633056640625\n",
      "77 1610.6068115234375\n",
      "78 1509.38330078125\n",
      "79 1414.986083984375\n",
      "80 1326.8648681640625\n",
      "81 1244.6004638671875\n",
      "82 1167.828125\n",
      "83 1096.0826416015625\n",
      "84 1029.0091552734375\n",
      "85 966.308349609375\n",
      "86 907.7164306640625\n",
      "87 852.8775634765625\n",
      "88 801.5714111328125\n",
      "89 753.552490234375\n",
      "90 708.5543212890625\n",
      "91 666.4148559570312\n",
      "92 626.9559936523438\n",
      "93 589.9710693359375\n",
      "94 555.299072265625\n",
      "95 522.7825927734375\n",
      "96 492.2837219238281\n",
      "97 463.68475341796875\n",
      "98 436.80743408203125\n",
      "99 411.5947265625\n",
      "100 387.9057312011719\n",
      "101 365.66033935546875\n",
      "102 344.7540283203125\n",
      "103 325.1112060546875\n",
      "104 306.65399169921875\n",
      "105 289.28692626953125\n",
      "106 272.9659118652344\n",
      "107 257.6034240722656\n",
      "108 243.14776611328125\n",
      "109 229.5590362548828\n",
      "110 216.7662811279297\n",
      "111 204.71438598632812\n",
      "112 193.36936950683594\n",
      "113 182.68690490722656\n",
      "114 172.62620544433594\n",
      "115 163.14511108398438\n",
      "116 154.20855712890625\n",
      "117 145.7836151123047\n",
      "118 137.8396453857422\n",
      "119 130.3492889404297\n",
      "120 123.28814697265625\n",
      "121 116.62297058105469\n",
      "122 110.33193969726562\n",
      "123 104.40061950683594\n",
      "124 98.80123138427734\n",
      "125 93.51094818115234\n",
      "126 88.52103424072266\n",
      "127 83.80714416503906\n",
      "128 79.35916900634766\n",
      "129 75.15408325195312\n",
      "130 71.18230438232422\n",
      "131 67.42991638183594\n",
      "132 63.88227844238281\n",
      "133 60.529998779296875\n",
      "134 57.360076904296875\n",
      "135 54.36274719238281\n",
      "136 51.52850341796875\n",
      "137 48.848079681396484\n",
      "138 46.313026428222656\n",
      "139 43.91118240356445\n",
      "140 41.63749694824219\n",
      "141 39.486820220947266\n",
      "142 37.45169448852539\n",
      "143 35.527042388916016\n",
      "144 33.70363235473633\n",
      "145 31.97728157043457\n",
      "146 30.342979431152344\n",
      "147 28.794904708862305\n",
      "148 27.328256607055664\n",
      "149 25.940570831298828\n",
      "150 24.625173568725586\n",
      "151 23.37881088256836\n",
      "152 22.198970794677734\n",
      "153 21.08016586303711\n",
      "154 20.019617080688477\n",
      "155 19.014484405517578\n",
      "156 18.061582565307617\n",
      "157 17.158504486083984\n",
      "158 16.301483154296875\n",
      "159 15.489532470703125\n",
      "160 14.71932315826416\n",
      "161 13.988911628723145\n",
      "162 13.294889450073242\n",
      "163 12.637543678283691\n",
      "164 12.013398170471191\n",
      "165 11.421363830566406\n",
      "166 10.858863830566406\n",
      "167 10.325843811035156\n",
      "168 9.819273948669434\n",
      "169 9.339293479919434\n",
      "170 8.8827543258667\n",
      "171 8.448921203613281\n",
      "172 8.037641525268555\n",
      "173 7.646667957305908\n",
      "174 7.2753801345825195\n",
      "175 6.922614097595215\n",
      "176 6.587757110595703\n",
      "177 6.26957368850708\n",
      "178 5.967332363128662\n",
      "179 5.679983139038086\n",
      "180 5.406866073608398\n",
      "181 5.147452354431152\n",
      "182 4.900491714477539\n",
      "183 4.665975570678711\n",
      "184 4.443099498748779\n",
      "185 4.2310991287231445\n",
      "186 4.029706001281738\n",
      "187 3.838092565536499\n",
      "188 3.6558992862701416\n",
      "189 3.4825146198272705\n",
      "190 3.3177194595336914\n",
      "191 3.1608684062957764\n",
      "192 3.011718273162842\n",
      "193 2.8695006370544434\n",
      "194 2.7345707416534424\n",
      "195 2.6060450077056885\n",
      "196 2.483816623687744\n",
      "197 2.3674347400665283\n",
      "198 2.2565810680389404\n",
      "199 2.1509015560150146\n",
      "200 2.0507190227508545\n",
      "201 1.9550518989562988\n",
      "202 1.8640007972717285\n",
      "203 1.777292251586914\n",
      "204 1.6948981285095215\n",
      "205 1.6162933111190796\n",
      "206 1.5412434339523315\n",
      "207 1.470008373260498\n",
      "208 1.4020919799804688\n",
      "209 1.3375186920166016\n",
      "210 1.275964617729187\n",
      "211 1.2172348499298096\n",
      "212 1.1612600088119507\n",
      "213 1.1080653667449951\n",
      "214 1.0572190284729004\n",
      "215 1.0088082551956177\n",
      "216 0.9627148509025574\n",
      "217 0.9187269806861877\n",
      "218 0.8768970370292664\n",
      "219 0.8370311856269836\n",
      "220 0.7988790273666382\n",
      "221 0.7625841498374939\n",
      "222 0.7278969287872314\n",
      "223 0.6949155926704407\n",
      "224 0.6634396910667419\n",
      "225 0.6333655714988708\n",
      "226 0.6047489047050476\n",
      "227 0.5774558186531067\n",
      "228 0.5514938831329346\n",
      "229 0.5265437364578247\n",
      "230 0.5029454231262207\n",
      "231 0.4803486466407776\n",
      "232 0.45882877707481384\n",
      "233 0.43816661834716797\n",
      "234 0.41852304339408875\n",
      "235 0.3997982144355774\n",
      "236 0.38196301460266113\n",
      "237 0.364889532327652\n",
      "238 0.34864872694015503\n",
      "239 0.3330940902233124\n",
      "240 0.3182850480079651\n",
      "241 0.30409175157546997\n",
      "242 0.2906179428100586\n",
      "243 0.2777474522590637\n",
      "244 0.2654135823249817\n",
      "245 0.2537044584751129\n",
      "246 0.24246087670326233\n",
      "247 0.23170314729213715\n",
      "248 0.22148610651493073\n",
      "249 0.2117132544517517\n",
      "250 0.20236501097679138\n",
      "251 0.19343844056129456\n",
      "252 0.184931218624115\n",
      "253 0.17680826783180237\n",
      "254 0.16905392706394196\n",
      "255 0.16162198781967163\n",
      "256 0.15454792976379395\n",
      "257 0.1477978378534317\n",
      "258 0.14133214950561523\n",
      "259 0.1351265162229538\n",
      "260 0.12923073768615723\n",
      "261 0.12358036637306213\n",
      "262 0.11820004880428314\n",
      "263 0.11303530633449554\n",
      "264 0.1081550195813179\n",
      "265 0.10344336926937103\n",
      "266 0.09893062710762024\n",
      "267 0.09462902694940567\n",
      "268 0.09052615612745285\n",
      "269 0.08661536872386932\n",
      "270 0.08288614451885223\n",
      "271 0.07930560410022736\n",
      "272 0.07583942264318466\n",
      "273 0.07258075475692749\n",
      "274 0.06942595541477203\n",
      "275 0.06644527614116669\n",
      "276 0.0635782927274704\n",
      "277 0.06084020435810089\n",
      "278 0.05823718011379242\n",
      "279 0.055741213262081146\n",
      "280 0.053352802991867065\n",
      "281 0.05105661600828171\n",
      "282 0.04884922504425049\n",
      "283 0.04675161466002464\n",
      "284 0.04475099965929985\n",
      "285 0.04285356402397156\n",
      "286 0.041014984250068665\n",
      "287 0.039275795221328735\n",
      "288 0.03760474547743797\n",
      "289 0.0360121950507164\n",
      "290 0.03448614850640297\n",
      "291 0.03301385045051575\n",
      "292 0.0316171795129776\n",
      "293 0.030269857496023178\n",
      "294 0.02897975593805313\n",
      "295 0.027748536318540573\n",
      "296 0.026585815474390984\n",
      "297 0.025473037734627724\n",
      "298 0.024392107501626015\n",
      "299 0.023369761183857918\n",
      "300 0.02238582633435726\n",
      "301 0.021432820707559586\n",
      "302 0.02053840644657612\n",
      "303 0.019674740731716156\n",
      "304 0.018850009888410568\n",
      "305 0.018056055530905724\n",
      "306 0.017309457063674927\n",
      "307 0.01658312976360321\n",
      "308 0.015893882140517235\n",
      "309 0.015233229845762253\n",
      "310 0.01459616981446743\n",
      "311 0.013999761082231998\n",
      "312 0.013412163592875004\n",
      "313 0.012851237319409847\n",
      "314 0.012330112047493458\n",
      "315 0.011827992275357246\n",
      "316 0.011339277029037476\n",
      "317 0.010873605497181416\n",
      "318 0.010433146730065346\n",
      "319 0.009998476132750511\n",
      "320 0.009590834379196167\n",
      "321 0.009207387454807758\n",
      "322 0.008830750361084938\n",
      "323 0.008470218628644943\n",
      "324 0.008131046779453754\n",
      "325 0.007803573273122311\n",
      "326 0.007488034665584564\n",
      "327 0.00718909315764904\n",
      "328 0.006903695873916149\n",
      "329 0.0066261691972613335\n",
      "330 0.006361581850796938\n",
      "331 0.006111744791269302\n",
      "332 0.005867352243512869\n",
      "333 0.005639410577714443\n",
      "334 0.005416968837380409\n",
      "335 0.005205124616622925\n",
      "336 0.004998964257538319\n",
      "337 0.004809095058590174\n",
      "338 0.004617080092430115\n",
      "339 0.004438231233507395\n",
      "340 0.004272385034710169\n",
      "341 0.004104608204215765\n",
      "342 0.003943478222936392\n",
      "343 0.0037927457597106695\n",
      "344 0.0036557272542268038\n",
      "345 0.00351382652297616\n",
      "346 0.0033798774238675833\n",
      "347 0.0032527961302548647\n",
      "348 0.003132424782961607\n",
      "349 0.0030160387977957726\n",
      "350 0.0029029017314314842\n",
      "351 0.002798874629661441\n",
      "352 0.002694320399314165\n",
      "353 0.0025957857724279165\n",
      "354 0.002505127340555191\n",
      "355 0.002417437732219696\n",
      "356 0.0023289257660508156\n",
      "357 0.002244621515274048\n",
      "358 0.002164731500670314\n",
      "359 0.002090973313897848\n",
      "360 0.0020174586679786444\n",
      "361 0.001947422861121595\n",
      "362 0.0018810220062732697\n",
      "363 0.001815801952034235\n",
      "364 0.0017541706329211593\n",
      "365 0.0016931532882153988\n",
      "366 0.0016352166421711445\n",
      "367 0.0015825015725567937\n",
      "368 0.0015290090814232826\n",
      "369 0.0014789438573643565\n",
      "370 0.0014303510542958975\n",
      "371 0.0013831304386258125\n",
      "372 0.0013375882990658283\n",
      "373 0.0012964424677193165\n",
      "374 0.0012539455201476812\n",
      "375 0.00121409073472023\n",
      "376 0.001173559227026999\n",
      "377 0.001138564432039857\n",
      "378 0.001103056245483458\n",
      "379 0.0010689453920349479\n",
      "380 0.001036228146404028\n",
      "381 0.0010031573474407196\n",
      "382 0.000973916205111891\n",
      "383 0.0009451770456507802\n",
      "384 0.0009150355472229421\n",
      "385 0.0008903736597858369\n",
      "386 0.0008627874776721001\n",
      "387 0.0008380786166526377\n",
      "388 0.000813718419522047\n",
      "389 0.0007908792467787862\n",
      "390 0.0007677742978557944\n",
      "391 0.0007468609255738556\n",
      "392 0.0007253302028402686\n",
      "393 0.0007060604402795434\n",
      "394 0.0006880201981402934\n",
      "395 0.0006683652172796428\n",
      "396 0.0006485966732725501\n",
      "397 0.0006305052665993571\n",
      "398 0.0006150761037133634\n",
      "399 0.0005981939611956477\n",
      "400 0.000582166772801429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 0.0005675730062648654\n",
      "402 0.0005527881439775229\n",
      "403 0.0005377176566980779\n",
      "404 0.0005248093511909246\n",
      "405 0.0005101532442495227\n",
      "406 0.0004967206041328609\n",
      "407 0.0004849989200010896\n",
      "408 0.0004724879690911621\n",
      "409 0.000460449984529987\n",
      "410 0.0004496993205975741\n",
      "411 0.00043695670319721103\n",
      "412 0.00042679053149186075\n",
      "413 0.0004164253477938473\n",
      "414 0.00040606240509077907\n",
      "415 0.00039667318924330175\n",
      "416 0.00038725879858247936\n",
      "417 0.000378115480998531\n",
      "418 0.0003690006269607693\n",
      "419 0.00036061694845557213\n",
      "420 0.00035245652543380857\n",
      "421 0.0003441825392656028\n",
      "422 0.0003375065280124545\n",
      "423 0.00032927756546996534\n",
      "424 0.00032215352985076606\n",
      "425 0.000315786455757916\n",
      "426 0.000308642367599532\n",
      "427 0.00030182438786141574\n",
      "428 0.0002948288747575134\n",
      "429 0.00028906919760629535\n",
      "430 0.00028285611188039184\n",
      "431 0.0002776266192086041\n",
      "432 0.00027133969706483185\n",
      "433 0.000264850037638098\n",
      "434 0.0002593512472230941\n",
      "435 0.0002545689349062741\n",
      "436 0.0002494208456482738\n",
      "437 0.0002444143174216151\n",
      "438 0.00023999436234589666\n",
      "439 0.0002346570836380124\n",
      "440 0.0002300886990269646\n",
      "441 0.0002251828118460253\n",
      "442 0.00022158707724884152\n",
      "443 0.0002175788104068488\n",
      "444 0.00021358499361667782\n",
      "445 0.00020927756850142032\n",
      "446 0.00020493274496402591\n",
      "447 0.00020101710106246173\n",
      "448 0.00019791984232142568\n",
      "449 0.000194107458810322\n",
      "450 0.00019035434524994344\n",
      "451 0.00018709333380684257\n",
      "452 0.00018364987045060843\n",
      "453 0.00017987150931730866\n",
      "454 0.00017684359045233577\n",
      "455 0.00017328518151771277\n",
      "456 0.00017043622210621834\n",
      "457 0.0001674266386544332\n",
      "458 0.00016455387230962515\n",
      "459 0.0001615206856513396\n",
      "460 0.0001592001790413633\n",
      "461 0.00015628083201590925\n",
      "462 0.00015367877495009452\n",
      "463 0.00015100406017154455\n",
      "464 0.00014879267837386578\n",
      "465 0.00014612700033467263\n",
      "466 0.00014369554992299527\n",
      "467 0.00014115642989054322\n",
      "468 0.0001387310476275161\n",
      "469 0.00013663437857758254\n",
      "470 0.0001345229393336922\n",
      "471 0.0001329147635260597\n",
      "472 0.00013070058776065707\n",
      "473 0.0001287628838326782\n",
      "474 0.00012687002890743315\n",
      "475 0.0001248102926183492\n",
      "476 0.0001229972840519622\n",
      "477 0.00012145691289333627\n",
      "478 0.00011957293463638052\n",
      "479 0.00011729532707249746\n",
      "480 0.00011566348985070363\n",
      "481 0.00011411477316869423\n",
      "482 0.00011226930655539036\n",
      "483 0.00011045779683627188\n",
      "484 0.00010859248141059652\n",
      "485 0.00010733633826021105\n",
      "486 0.00010599999222904444\n",
      "487 0.00010431461851112545\n",
      "488 0.00010284481686539948\n",
      "489 0.00010148289584321901\n",
      "490 0.00010024341463577002\n",
      "491 9.87908206298016e-05\n",
      "492 9.742921247379854e-05\n",
      "493 9.636324102757499e-05\n",
      "494 9.495772974332795e-05\n",
      "495 9.362151467939839e-05\n",
      "496 9.250194125343114e-05\n",
      "497 9.122757910517976e-05\n",
      "498 8.997998520499095e-05\n",
      "499 8.847222488839179e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
