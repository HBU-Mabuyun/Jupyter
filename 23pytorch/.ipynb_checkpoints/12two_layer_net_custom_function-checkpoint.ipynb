{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining new autograd functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31368516.0\n",
      "1 27116342.0\n",
      "2 26994252.0\n",
      "3 26711260.0\n",
      "4 23696660.0\n",
      "5 18024618.0\n",
      "6 11763796.0\n",
      "7 6974952.0\n",
      "8 4040564.0\n",
      "9 2451887.75\n",
      "10 1620544.875\n",
      "11 1171665.75\n",
      "12 909745.4375\n",
      "13 741265.6875\n",
      "14 622273.3125\n",
      "15 531981.625\n",
      "16 459972.59375\n",
      "17 400775.65625\n",
      "18 351157.3125\n",
      "19 309089.28125\n",
      "20 273072.5\n",
      "21 242047.03125\n",
      "22 215222.0\n",
      "23 191908.328125\n",
      "24 171558.109375\n",
      "25 153713.8125\n",
      "26 138014.609375\n",
      "27 124164.15625\n",
      "28 111907.875\n",
      "29 101041.40625\n",
      "30 91367.4765625\n",
      "31 82750.0\n",
      "32 75056.9609375\n",
      "33 68176.8515625\n",
      "34 62007.78515625\n",
      "35 56462.8359375\n",
      "36 51458.109375\n",
      "37 46949.25390625\n",
      "38 42878.80078125\n",
      "39 39199.65625\n",
      "40 35875.78125\n",
      "41 32872.98046875\n",
      "42 30150.13671875\n",
      "43 27674.638671875\n",
      "44 25423.931640625\n",
      "45 23373.302734375\n",
      "46 21502.181640625\n",
      "47 19794.759765625\n",
      "48 18235.1953125\n",
      "49 16808.55859375\n",
      "50 15503.115234375\n",
      "51 14307.4345703125\n",
      "52 13211.0234375\n",
      "53 12205.919921875\n",
      "54 11282.8203125\n",
      "55 10434.869140625\n",
      "56 9655.3740234375\n",
      "57 8938.619140625\n",
      "58 8278.58984375\n",
      "59 7670.88232421875\n",
      "60 7110.953125\n",
      "61 6594.8525390625\n",
      "62 6118.572265625\n",
      "63 5679.66552734375\n",
      "64 5275.1396484375\n",
      "65 4901.58154296875\n",
      "66 4556.39404296875\n",
      "67 4237.0810546875\n",
      "68 3941.513671875\n",
      "69 3667.76953125\n",
      "70 3414.118896484375\n",
      "71 3179.02294921875\n",
      "72 2961.05615234375\n",
      "73 2758.906005859375\n",
      "74 2571.3095703125\n",
      "75 2397.1435546875\n",
      "76 2235.4326171875\n",
      "77 2085.245361328125\n",
      "78 1945.6630859375\n",
      "79 1815.882080078125\n",
      "80 1695.196044921875\n",
      "81 1582.93408203125\n",
      "82 1478.4766845703125\n",
      "83 1381.2266845703125\n",
      "84 1290.728515625\n",
      "85 1206.435302734375\n",
      "86 1127.970703125\n",
      "87 1054.775390625\n",
      "88 986.551025390625\n",
      "89 922.9249877929688\n",
      "90 863.5707397460938\n",
      "91 808.2041625976562\n",
      "92 756.538330078125\n",
      "93 708.3099365234375\n",
      "94 663.3092041015625\n",
      "95 621.2771606445312\n",
      "96 582.0648193359375\n",
      "97 545.4089965820312\n",
      "98 511.1683044433594\n",
      "99 479.14898681640625\n",
      "100 449.2054748535156\n",
      "101 421.20050048828125\n",
      "102 395.0164794921875\n",
      "103 370.517578125\n",
      "104 347.5874328613281\n",
      "105 326.1288757324219\n",
      "106 306.0368957519531\n",
      "107 287.2414245605469\n",
      "108 269.6257629394531\n",
      "109 253.1292266845703\n",
      "110 237.674072265625\n",
      "111 223.18966674804688\n",
      "112 209.6165771484375\n",
      "113 196.8965301513672\n",
      "114 184.97286987304688\n",
      "115 173.79185485839844\n",
      "116 163.30712890625\n",
      "117 153.47747802734375\n",
      "118 144.2567138671875\n",
      "119 135.60491943359375\n",
      "120 127.48678588867188\n",
      "121 119.86914825439453\n",
      "122 112.71845245361328\n",
      "123 106.00975036621094\n",
      "124 99.7094955444336\n",
      "125 93.79347229003906\n",
      "126 88.23567962646484\n",
      "127 83.01862335205078\n",
      "128 78.12187194824219\n",
      "129 73.51661682128906\n",
      "130 69.19043731689453\n",
      "131 65.12671661376953\n",
      "132 61.30558776855469\n",
      "133 57.71683883666992\n",
      "134 54.34255599975586\n",
      "135 51.16942596435547\n",
      "136 48.186161041259766\n",
      "137 45.38166427612305\n",
      "138 42.74386215209961\n",
      "139 40.26268005371094\n",
      "140 37.929161071777344\n",
      "141 35.733489990234375\n",
      "142 33.66770553588867\n",
      "143 31.725120544433594\n",
      "144 29.897384643554688\n",
      "145 28.176097869873047\n",
      "146 26.5561466217041\n",
      "147 25.031599044799805\n",
      "148 23.59637451171875\n",
      "149 22.246309280395508\n",
      "150 20.974321365356445\n",
      "151 19.77684211730957\n",
      "152 18.648113250732422\n",
      "153 17.586029052734375\n",
      "154 16.585186004638672\n",
      "155 15.642463684082031\n",
      "156 14.754363059997559\n",
      "157 13.917581558227539\n",
      "158 13.129422187805176\n",
      "159 12.386725425720215\n",
      "160 11.686697959899902\n",
      "161 11.027087211608887\n",
      "162 10.405202865600586\n",
      "163 9.819051742553711\n",
      "164 9.266730308532715\n",
      "165 8.746231079101562\n",
      "166 8.255226135253906\n",
      "167 7.792117595672607\n",
      "168 7.355751991271973\n",
      "169 6.944152355194092\n",
      "170 6.5561981201171875\n",
      "171 6.189929008483887\n",
      "172 5.844435214996338\n",
      "173 5.51853084564209\n",
      "174 5.21138858795166\n",
      "175 4.9212164878845215\n",
      "176 4.647850036621094\n",
      "177 4.389738082885742\n",
      "178 4.146340847015381\n",
      "179 3.9166789054870605\n",
      "180 3.6996676921844482\n",
      "181 3.4950308799743652\n",
      "182 3.301942825317383\n",
      "183 3.119563341140747\n",
      "184 2.947589159011841\n",
      "185 2.785073757171631\n",
      "186 2.6316933631896973\n",
      "187 2.486912727355957\n",
      "188 2.3501062393188477\n",
      "189 2.2208969593048096\n",
      "190 2.098975419998169\n",
      "191 1.9841670989990234\n",
      "192 1.8754626512527466\n",
      "193 1.7726942300796509\n",
      "194 1.6756856441497803\n",
      "195 1.5840777158737183\n",
      "196 1.4976599216461182\n",
      "197 1.415900707244873\n",
      "198 1.3386280536651611\n",
      "199 1.2656903266906738\n",
      "200 1.196766972541809\n",
      "201 1.131651759147644\n",
      "202 1.0701446533203125\n",
      "203 1.0120208263397217\n",
      "204 0.956977128982544\n",
      "205 0.9051734805107117\n",
      "206 0.8560673594474792\n",
      "207 0.8096973896026611\n",
      "208 0.7658828496932983\n",
      "209 0.7244935631752014\n",
      "210 0.6853047609329224\n",
      "211 0.6482803225517273\n",
      "212 0.6132906675338745\n",
      "213 0.5802164077758789\n",
      "214 0.5489041805267334\n",
      "215 0.519362211227417\n",
      "216 0.4914129674434662\n",
      "217 0.4649834632873535\n",
      "218 0.4399796426296234\n",
      "219 0.41629594564437866\n",
      "220 0.393967866897583\n",
      "221 0.3728031516075134\n",
      "222 0.3528421223163605\n",
      "223 0.33390381932258606\n",
      "224 0.3160144090652466\n",
      "225 0.2991563677787781\n",
      "226 0.28315791487693787\n",
      "227 0.26805582642555237\n",
      "228 0.25368770956993103\n",
      "229 0.24011419713497162\n",
      "230 0.22728660702705383\n",
      "231 0.21514275670051575\n",
      "232 0.20365560054779053\n",
      "233 0.19281448423862457\n",
      "234 0.18253767490386963\n",
      "235 0.17283658683300018\n",
      "236 0.1636410653591156\n",
      "237 0.15493297576904297\n",
      "238 0.14669878780841827\n",
      "239 0.13890841603279114\n",
      "240 0.13156180083751678\n",
      "241 0.12454967200756073\n",
      "242 0.11797694116830826\n",
      "243 0.11173246055841446\n",
      "244 0.10582847148180008\n",
      "245 0.1002161055803299\n",
      "246 0.09493035078048706\n",
      "247 0.0898907259106636\n",
      "248 0.08514600247144699\n",
      "249 0.08063112199306488\n",
      "250 0.07640815526247025\n",
      "251 0.07237044721841812\n",
      "252 0.06855686753988266\n",
      "253 0.06494025141000748\n",
      "254 0.06149737909436226\n",
      "255 0.0582708977162838\n",
      "256 0.05520588904619217\n",
      "257 0.052298445254564285\n",
      "258 0.04955678433179855\n",
      "259 0.04693238064646721\n",
      "260 0.04447657987475395\n",
      "261 0.04215504601597786\n",
      "262 0.03993411734700203\n",
      "263 0.03784150630235672\n",
      "264 0.03585812821984291\n",
      "265 0.03398573398590088\n",
      "266 0.032208312302827835\n",
      "267 0.03051113709807396\n",
      "268 0.02892288565635681\n",
      "269 0.02741198055446148\n",
      "270 0.02598668821156025\n",
      "271 0.024624083191156387\n",
      "272 0.023351026698946953\n",
      "273 0.022135432809591293\n",
      "274 0.02098865434527397\n",
      "275 0.019889958202838898\n",
      "276 0.01885758712887764\n",
      "277 0.017880016937851906\n",
      "278 0.016957774758338928\n",
      "279 0.016082916408777237\n",
      "280 0.015258857049047947\n",
      "281 0.014471150934696198\n",
      "282 0.013729710131883621\n",
      "283 0.013023962266743183\n",
      "284 0.012356509454548359\n",
      "285 0.01173092145472765\n",
      "286 0.011122221127152443\n",
      "287 0.010565602220594883\n",
      "288 0.01002822071313858\n",
      "289 0.009520910680294037\n",
      "290 0.0090325977653265\n",
      "291 0.00857721921056509\n",
      "292 0.008147654123604298\n",
      "293 0.007739017717540264\n",
      "294 0.007347309496253729\n",
      "295 0.0069832634180784225\n",
      "296 0.006633090320974588\n",
      "297 0.006296149920672178\n",
      "298 0.00598547188565135\n",
      "299 0.0056845033541321754\n",
      "300 0.005406457465142012\n",
      "301 0.0051447018049657345\n",
      "302 0.004888925701379776\n",
      "303 0.004653272219002247\n",
      "304 0.004427465610206127\n",
      "305 0.00421264348551631\n",
      "306 0.00401033740490675\n",
      "307 0.0038159277755767107\n",
      "308 0.0036353771574795246\n",
      "309 0.003462651977315545\n",
      "310 0.0033005671575665474\n",
      "311 0.003143122186884284\n",
      "312 0.0029944968409836292\n",
      "313 0.002855513244867325\n",
      "314 0.0027219646144658327\n",
      "315 0.0025967659894376993\n",
      "316 0.0024806561414152384\n",
      "317 0.002365271793678403\n",
      "318 0.0022578954230993986\n",
      "319 0.002158855553716421\n",
      "320 0.0020610312931239605\n",
      "321 0.001970351906493306\n",
      "322 0.0018822526326403022\n",
      "323 0.0018010196508839726\n",
      "324 0.0017203913303092122\n",
      "325 0.0016453319694846869\n",
      "326 0.0015737510984763503\n",
      "327 0.0015073451213538647\n",
      "328 0.001442054403014481\n",
      "329 0.0013815452111884952\n",
      "330 0.0013231681659817696\n",
      "331 0.0012703572865575552\n",
      "332 0.0012186630629003048\n",
      "333 0.0011666439240798354\n",
      "334 0.001120170229114592\n",
      "335 0.0010764929465949535\n",
      "336 0.001033495762385428\n",
      "337 0.0009924531914293766\n",
      "338 0.0009531618561595678\n",
      "339 0.000916323100682348\n",
      "340 0.0008811273146420717\n",
      "341 0.0008455849601887167\n",
      "342 0.0008129483321681619\n",
      "343 0.0007832677802070975\n",
      "344 0.0007529047434218228\n",
      "345 0.0007251846836879849\n",
      "346 0.0006981089827604592\n",
      "347 0.000672767055220902\n",
      "348 0.0006476640701293945\n",
      "349 0.000625037879217416\n",
      "350 0.0006033281097188592\n",
      "351 0.0005800556973554194\n",
      "352 0.0005590189248323441\n",
      "353 0.000540409586392343\n",
      "354 0.000521976500749588\n",
      "355 0.0005028130253776908\n",
      "356 0.00048514342051930726\n",
      "357 0.00046941390610300004\n",
      "358 0.0004520034708548337\n",
      "359 0.00043718243250623345\n",
      "360 0.00042269754339940846\n",
      "361 0.0004096529446542263\n",
      "362 0.00039637807640247047\n",
      "363 0.0003825708699878305\n",
      "364 0.0003707697323989123\n",
      "365 0.0003586130915209651\n",
      "366 0.0003465800837147981\n",
      "367 0.00033653859281912446\n",
      "368 0.00032544974237680435\n",
      "369 0.00031631020829081535\n",
      "370 0.0003065673809032887\n",
      "371 0.0002968545304611325\n",
      "372 0.0002876358921639621\n",
      "373 0.0002794299216475338\n",
      "374 0.00027071754448115826\n",
      "375 0.0002630369854159653\n",
      "376 0.00025518948677927256\n",
      "377 0.000247742427745834\n",
      "378 0.00024059490533545613\n",
      "379 0.00023379785125143826\n",
      "380 0.0002275497536174953\n",
      "381 0.0002214095729868859\n",
      "382 0.00021486375771928579\n",
      "383 0.00020903591939713806\n",
      "384 0.00020369880076032132\n",
      "385 0.0001982348330784589\n",
      "386 0.00019329441420268267\n",
      "387 0.0001880465424619615\n",
      "388 0.00018374499632045627\n",
      "389 0.00017835614562500268\n",
      "390 0.0001732852979330346\n",
      "391 0.00016878970200195909\n",
      "392 0.000163885168149136\n",
      "393 0.00016029214020818472\n",
      "394 0.00015630711277481169\n",
      "395 0.00015168110257945955\n",
      "396 0.00014778855256736279\n",
      "397 0.0001442655484424904\n",
      "398 0.0001404363283654675\n",
      "399 0.0001373365375911817\n",
      "400 0.00013358374417293817\n",
      "401 0.00013022117491345853\n",
      "402 0.00012717592471744865\n",
      "403 0.00012391479685902596\n",
      "404 0.00012124892964493483\n",
      "405 0.00011880785314133391\n",
      "406 0.00011610589717747644\n",
      "407 0.00011354428716003895\n",
      "408 0.00011082685523433611\n",
      "409 0.00010811837273649871\n",
      "410 0.00010607316653477028\n",
      "411 0.0001035579334711656\n",
      "412 0.00010148507135454565\n",
      "413 9.920022421283647e-05\n",
      "414 9.695399785414338e-05\n",
      "415 9.519815648673102e-05\n",
      "416 9.295793279306963e-05\n",
      "417 9.108932135859504e-05\n",
      "418 8.886602154234424e-05\n",
      "419 8.74059769557789e-05\n",
      "420 8.516076923115179e-05\n",
      "421 8.361075015272945e-05\n",
      "422 8.18625048850663e-05\n",
      "423 8.0306657764595e-05\n",
      "424 7.881542114773765e-05\n",
      "425 7.730827201157808e-05\n",
      "426 7.569105946458876e-05\n",
      "427 7.45157667552121e-05\n",
      "428 7.308534986805171e-05\n",
      "429 7.180139073170722e-05\n",
      "430 7.048965926514938e-05\n",
      "431 6.883071910124272e-05\n",
      "432 6.760709948139265e-05\n",
      "433 6.605492671951652e-05\n",
      "434 6.490377563750371e-05\n",
      "435 6.398336699930951e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436 6.25054890406318e-05\n",
      "437 6.147770181996748e-05\n",
      "438 6.0576530813705176e-05\n",
      "439 5.935509761911817e-05\n",
      "440 5.855140261701308e-05\n",
      "441 5.7672052207635716e-05\n",
      "442 5.6561890232842416e-05\n",
      "443 5.56758968741633e-05\n",
      "444 5.463084380608052e-05\n",
      "445 5.3761563322041184e-05\n",
      "446 5.270666952128522e-05\n",
      "447 5.198900180403143e-05\n",
      "448 5.108256664243527e-05\n",
      "449 5.025399150326848e-05\n",
      "450 4.944618194713257e-05\n",
      "451 4.869233089266345e-05\n",
      "452 4.777517824550159e-05\n",
      "453 4.683215593104251e-05\n",
      "454 4.6289067540783435e-05\n",
      "455 4.562174581224099e-05\n",
      "456 4.499458009377122e-05\n",
      "457 4.425035876920447e-05\n",
      "458 4.371292016003281e-05\n",
      "459 4.2987914639525115e-05\n",
      "460 4.2368043068563566e-05\n",
      "461 4.1737275751074776e-05\n",
      "462 4.121288293390535e-05\n",
      "463 4.063494270667434e-05\n",
      "464 3.9895061490824446e-05\n",
      "465 3.942671901313588e-05\n",
      "466 3.869970532832667e-05\n",
      "467 3.807366010732949e-05\n",
      "468 3.7536578020080924e-05\n",
      "469 3.7137953768251464e-05\n",
      "470 3.672583989100531e-05\n",
      "471 3.6165791243547574e-05\n",
      "472 3.5642475268105045e-05\n",
      "473 3.512912371661514e-05\n",
      "474 3.457152342889458e-05\n",
      "475 3.411329089431092e-05\n",
      "476 3.370528429513797e-05\n",
      "477 3.318894232506864e-05\n",
      "478 3.282077523181215e-05\n",
      "479 3.2371382985729724e-05\n",
      "480 3.1911924452288076e-05\n",
      "481 3.1488558306591585e-05\n",
      "482 3.115612344117835e-05\n",
      "483 3.0930808861739933e-05\n",
      "484 3.04807963402709e-05\n",
      "485 3.029564322787337e-05\n",
      "486 2.990925349877216e-05\n",
      "487 2.960126948892139e-05\n",
      "488 2.9304635972948745e-05\n",
      "489 2.886162474169396e-05\n",
      "490 2.862494147848338e-05\n",
      "491 2.8188842406962067e-05\n",
      "492 2.794835381791927e-05\n",
      "493 2.7496720576891676e-05\n",
      "494 2.7166612198925577e-05\n",
      "495 2.694627437449526e-05\n",
      "496 2.6562407583696768e-05\n",
      "497 2.6296729629393667e-05\n",
      "498 2.5952587748179212e-05\n",
      "499 2.5804021788644604e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
