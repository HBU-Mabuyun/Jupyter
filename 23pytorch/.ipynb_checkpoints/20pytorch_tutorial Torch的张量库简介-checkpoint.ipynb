{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction to PyTorch\n",
    "***********************\n",
    "\n",
    "Introduction to Torch's tensor library\n",
    "======================================\n",
    "\n",
    "All of deep learning is computations on tensors, which are generalizations of a matrix that can be indexed in more than 2 dimensions. \n",
    "\n",
    "所有的深度学习都是关于张量的计算，它一般是矩阵可以被索引到高于2维。\n",
    "\n",
    "We will see exactly what this means in-depth later. \n",
    "\n",
    "我们稍后将详细讨论这意味着什么。\n",
    "\n",
    "First,lets look what we can do with tensors.\n",
    "\n",
    "首先，让我们看看我们能用张量做什么。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f749220370>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Tensors\n",
    "# 创建张量\n",
    "\n",
    "Tensors can be created from Python lists with the torch.Tensor()function.\n",
    "\n",
    "\n",
    "张量可以从Python列表中使用torch.Tensor()函数创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是三维张量？\n",
    "\n",
    "像这样思考。\n",
    "\n",
    "如果你有一个向量，对向量的索引会给你一个标量。\n",
    "\n",
    "如果你有一个矩阵，对矩阵的索引会给你一个向量。\n",
    "\n",
    "如果你有一个三维张量，那么对张量的索引就会得到一个矩阵！\n",
    "\n",
    "## 注意术语:\n",
    "\n",
    "当我在本教程中说“张量”时，它指的是任何torch.Tensor对象。\n",
    "\n",
    "矩阵和向量是torch的特殊情况。\n",
    "\n",
    "张量，它们的维数分别是1和2。\n",
    "\n",
    "当我说到3D张量时，我将明确地使用“3D张量”这个术语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 3.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以创建其他数据类型的张量。默认值，如您所见，是Float。\n",
    "\n",
    "要创建整数类型的张量，请尝试一下torch.LongTensor()\n",
    "\n",
    "检查文档以获得更多的数据类型，但是Float和Long将是最常见的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以用随机数和提供的维度来创建一个张量:torch.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
      "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
      "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
      "\n",
      "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
      "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
      "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
      "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
      "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations with Tensors\n",
    "\n",
    "你可以按照你期望的方式对张量进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `the documentation <http://pytorch.org/docs/torch.html>`__ for a complete list of the massive number of operations available to you. \n",
    "\n",
    "They expand beyond just mathematical operations.\n",
    "\n",
    "它们的扩展 比数学运算还要多。\n",
    "\n",
    "One helpful operation that we will make use of later is concatenation.\n",
    "\n",
    "我们稍后将使用的一个有用的操作是串联。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 默认按照 行 连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n",
      "        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n",
      "        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n",
      "        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n",
      "        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照 列 连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,\n",
      "          1.2381],\n",
      "        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668,\n",
      "         -0.9999]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors 重塑张量\n",
    "\n",
    "Use the .view() method to reshape a tensor. \n",
    "\n",
    "This method receives heavy use, because many neural network components expect their inputs to have a certain shape. \n",
    "\n",
    "这种方法得到了大量的使用，因为许多神经网络组件期望它们的输入有一定的形状。\n",
    "\n",
    "Often you will need to reshape before passing your data to the component.\n",
    "\n",
    "通常，在将数据传递给组件之前，您需要进行重构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n",
      "         [-0.6240, -0.9773,  0.8748,  0.9873],\n",
      "         [-0.0594, -2.4919,  0.2423,  0.2883]],\n",
      "\n",
      "        [[-0.1095,  0.3126,  1.5038,  0.5038],\n",
      "         [ 0.6223, -0.4481, -0.2856,  0.3880],\n",
      "         [-1.1435, -0.6512, -0.1032,  0.6937]]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,\n",
      "          0.9873, -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,\n",
      "          0.3880, -1.1435, -0.6512, -0.1032,  0.6937]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,\n",
      "          0.9873, -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,\n",
      "          0.3880, -1.1435, -0.6512, -0.1032,  0.6937]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Graphs and Automatic Differentiation 计算图和自动微分\n",
    "================================================\n",
    "\n",
    "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write the\n",
    "back propagation gradients yourself. \n",
    "\n",
    "计算图的概念对于有效的深度学习编程是至关重要的，因为它允许您不必自己编写反向传播梯度。\n",
    "\n",
    "A computation graph is simply a specification of how your data is combined to give you the output. \n",
    "\n",
    "计算图只是说明如何将数据组合起来以获得输出的规范。\n",
    "\n",
    "Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
    "\n",
    "由于图形完全指定了哪些参数与哪些操作有关，所以它包含了足够的信息来计算导数。\n",
    "\n",
    "This probably sounds vague, so let's see what is going on using the fundamental flag ``requires_grad``.\n",
    "\n",
    "这可能听起来很模糊，所以让我们看看使用基本标志``requires_grad``的情况\n",
    "\n",
    "First, think from a programmers perspective. \n",
    "\n",
    "首先，从程序员的角度来看。\n",
    "\n",
    "What is stored in the torch.\n",
    "\n",
    "在torch中储存了什么。\n",
    "\n",
    "Tensor objects we were creating above? \n",
    "\n",
    "我们在上面创建的张量对象？\n",
    "\n",
    "Obviously the data and the shape, and maybe a few other things. \n",
    "\n",
    "很明显，数据和形状，还有一些其他的东西。\n",
    "\n",
    "But when we added two tensors together, we got an output tensor. \n",
    "\n",
    "但是当我们把两个张量加在一起时，我们得到了一个输出张量。\n",
    "\n",
    "All this output tensor knows is its data and shape. \n",
    "\n",
    "所有这些输出张量都知道它的数据和形状。\n",
    "\n",
    "It has no idea that it was the sum of two other tensors (it could have been read in from a file, it could be the result of some\n",
    "other operation, etc.)\n",
    "\n",
    "它不知道它是另外两个张量的和（它可能是从文件中读取的，可能是一些文件的结果其他操作,等等)。\n",
    "\n",
    "If ``requires_grad=True``, the Tensor object keeps track of how it was created. \n",
    "\n",
    "If ``requires_grad=True``,张量对象跟踪它是如何被创建的。\n",
    "\n",
    "Lets see it in action.\n",
    "\n",
    "\n",
    "让我们在行动中看到它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n",
      "<AddBackward1 object at 0x000001F74B564828>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "\n",
    "# With requires_grad=True, you can still do all the operations you previously could\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Tensors know what created them. \n",
    "\n",
    "所以张量知道是什么创造了它们。\n",
    "\n",
    "z knows that it wasn't read in from a file, it wasn't the result of a multiplication or exponential or whatever. \n",
    "\n",
    "z知道它不是从一个文件中读取的，它不是一个乘法或指数的结果，或者别的什么。\n",
    "\n",
    "And if you keep following z.grad_fn, you will find yourself at x and y.\n",
    "\n",
    "如果你一直跟着z。你会在x和y发现自己。\n",
    "\n",
    "But how does that help us compute a gradient?\n",
    "\n",
    "但这如何帮助我们计算梯度呢？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.)\n",
      "<SumBackward0 object at 0x000001F74B564518>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, what is the derivative of this sum with respect to the first component of x? \n",
    "\n",
    "那么现在，x的第一个分量的和的导数是什么呢？\n",
    "\n",
    "In math, we want\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "\n",
    "Well, s knows that it was created as a sum of the tensor z. z knows that it was the sum x + y. \n",
    "\n",
    "嗯，s知道它是作为张量z的和。z知道它是x+y的总和。\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "And so s contains enough information to determine that the derivative we want is 1!\n",
    "\n",
    "所以s包含了足够的信息来确定我们想要的导数是1！\n",
    "\n",
    "Of course this glosses over the challenge of how to actually compute that derivative. \n",
    "\n",
    "当然，这掩盖了如何计算导数的挑战。\n",
    "\n",
    "The point here is that s is carrying along enough information that it is possible to compute it. \n",
    "\n",
    "这里的重点是s携带了足够的信息，可以计算它。\n",
    "\n",
    "In reality, the developers of Pytorch program the sum() and + operations to know how to compute their gradients, and run the back propagation algorithm. \n",
    "\n",
    "在现实中，Pytorch程序的开发人员可以通过sum（）和+操作来计算他们的梯度，并运行反向传播算法。\n",
    "\n",
    "An in-depth discussion of that algorithm is beyond the scope of this tutorial.\n",
    "\n",
    "对该算法的深入讨论超出了本教程的范围。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have Pytorch compute the gradient, and see that we were right:\n",
    "\n",
    "让Pytorch计算梯度，看看我们是对的：\n",
    "\n",
    "(note if you run this block multiple times, the gradient will increment.\n",
    "\n",
    "注意，如果你多次运行这个block，梯度将会增加。\n",
    "\n",
    "That is because Pytorch *accumulates* the gradient into the .grad property, since for many models this is very convenient.)\n",
    "\n",
    "这是因为Pytorch将梯度累积到.grad的属性中，因为对于许多模型来说，这是非常方便的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.,  6.,  6.])\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going on in the block below is crucial for being a successful programmer in deep learning.\n",
    "\n",
    "\n",
    "# 理解下面这一段的内容对于成为一个成功的深度学习的程序员是至关重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001F74B564240>\n"
     ]
    }
   ],
   "source": [
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad`` flag in-place. \n",
    "#  The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "# ... does new_z have information to backprop to x and y? # NO!\n",
    "print(new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage as ``z``, but with the computation history forgotten. \n",
    "#  It doesn't know anything about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stops autograd from tracking history on Tensors with requires_grad=True by wrapping the code block in ``with torch.no_grad():``\n",
    "\n",
    "你也可以阻止autograd在张量上跟踪历史requiresgrad=True，用结束代码块``with torch.no_grad():``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
