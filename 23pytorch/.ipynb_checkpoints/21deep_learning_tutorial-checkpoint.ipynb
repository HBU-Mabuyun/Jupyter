{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep Learning with PyTorch\n",
    "**************************\n",
    "\n",
    "Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
    "深度学习构建模块：仿射映射、非线性和对象\n",
    "==========================================================================\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities in clever ways. \n",
    "\n",
    "深度学习包含以巧妙的方式组合的非线性的线性关系。\n",
    "\n",
    "The introduction of non-linearities allows for powerful models. \n",
    "\n",
    "非线性的引入允许有强大的模型。\n",
    "\n",
    "In this section, we will play with these core components, make up an objective function, and see how the model is trained.\n",
    "\n",
    "在本节中，我们将使用这些核心组件，组成一个目标函数，并查看模型是如何被训练的。\n",
    "\n",
    "Affine Maps\n",
    "## 仿射映射\n",
    "\n",
    "### 一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。\n",
    "\n",
    "One of the core workhorses of deep learning is the affine map, which is a function $f(x)$ where\n",
    "\n",
    "深度学习的核心工作之一是仿射映射，它是一个函数$f(x)$：\n",
    "\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be learned here are $A$ and $b$. \n",
    "\n",
    "对于一个矩阵$A$和向量$x$，$b$。这里要学习的参数是$A$和$b$。\n",
    "\n",
    "Often, $b$ is refered to as the *bias* term.\n",
    "\n",
    "通常，$b$被用为$偏差$项。\n",
    "\n",
    "PyTorch and most other deep learning frameworks do things a little differently than traditional linear algebra. \n",
    "\n",
    "PyTorch和大多数其他深度学习框架的做法，与传统的线性代数略有不同。\n",
    "\n",
    "It maps the rows of the input instead of the columns. \n",
    "\n",
    "它映射输入的行而不是列。\n",
    "\n",
    "That is, the $i$'th row of the output below is the mapping of the $i$'th row of the input under $A$, plus the bias term. \n",
    "\n",
    "也就是说，输出的第$i$行下面，是在$A$下面的第$i$行的映射，加上偏差项。\n",
    "\n",
    "Look at the example below.\n",
    "\n",
    "\n",
    "请看下面的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x166a6e195f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4107, -0.9880, -0.9081,  0.5423,  0.1103],\n",
       "        [-2.2590,  0.6067, -0.1383,  0.8310, -0.2477]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = torch.randn(2, 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2889,  0.3574,  0.6554],\n",
       "        [-0.9682,  0.0289,  0.4426]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(data)  # yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Linearities\n",
    "# 非线性关系\n",
    "\n",
    "First, note the following fact, which will explain why we need non-linearities in the first place. \n",
    "\n",
    "首先，注意以下事实，这将解释：为什么我们首先需要非线性。\n",
    "\n",
    "Suppose we have two affine maps\n",
    "\n",
    "假设我们有两个仿射变换\n",
    "\n",
    "$f(x) = Ax + b$   和  $g(x) = Cx + d$. \n",
    "\n",
    "What is  $f(g(x))$?\n",
    "\n",
    "\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n",
    "\n",
    "$AC$ is a matrix and $Ad + b$ is a vector, so we see that composing affine maps gives you an affine map.\n",
    "\n",
    "所以我们看到，合成仿射映射会给你一个仿射映射\n",
    "\n",
    "From this, you can see that if you wanted your neural network to be long chains of affine compositions, that this adds no new power to your model than just doing a single affine map.\n",
    "\n",
    "由此，你可以看到，如果你想让你的神经网络成为一长串的仿射组合，那么，比起只做一个仿射映射，这对你的模型来说并没有什么新的力量。\n",
    "\n",
    "If we introduce non-linearities in between the affine layers, this is no longer the case, and we can build much more powerful models.\n",
    "\n",
    "如果我们在仿射层之间引入非线性，现在已经不是这样了，我们可以构建更强大的模型。\n",
    "\n",
    "There are a few core non-linearities.\n",
    "\n",
    "有一些核心的非线性。\n",
    "\n",
    "$\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ are the most common. 是最常见的。\n",
    "\n",
    "You are probably wondering: \"why these functions? I can think of plenty of other non-linearities.\" \n",
    "\n",
    "你可能在想：“为什么这些函数？我能想到很多其他的非线性。”\n",
    "\n",
    "The reason for this is that they have gradients that are easy to compute, and computing gradients is essential for learning.\n",
    "\n",
    "这样做的原因是它们的梯度很容易计算，而计算梯度对于学习是必不可少的。\n",
    "\n",
    "For example\n",
    "\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n",
    "A quick note: \n",
    "\n",
    "although you may have learned some neural networks in your intro to AI class where $\\sigma(x)$ was the default non-linearity,typically people shy away from it in practice. \n",
    "\n",
    "尽管你可能已经在AI课程的介绍中学习了一些神经网络，在这里，$sigma（x）$是默认的非线性，通常人们在实践中会回避它。\n",
    "\n",
    "This is because the gradient *vanishes* very quickly as the absolute value of the argument grows. \n",
    "\n",
    "这是因为当参数的绝对值增加时，梯度就会很快消失。\n",
    "\n",
    "Small gradients means it is hard to learn.\n",
    "\n",
    "小的梯度意味着它很难学。\n",
    "\n",
    "Most people default to tanh or ReLU.\n",
    "\n",
    "大多数人默认用tanh或ReLU。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8029,  0.2366],\n",
      "        [ 0.2857,  0.6898]])\n",
      "tensor([[ 0.0000,  0.2366],\n",
      "        [ 0.2857,  0.6898]])\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = torch.randn(2, 2)\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and Probabilities\n",
    "\n",
    "\n",
    "The function $\\text{Softmax}(x)$ is also just a non-linearity, but it is special in that it usually is the last operation done in a\n",
    "network. \n",
    "\n",
    "This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. \n",
    "\n",
    "Let $x$ be a vector of real numbers (positive, negative, whatever,there are no constraints). \n",
    "\n",
    "Then the i'th component of $\\text{Softmax}(x)$ is \\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n",
    "\n",
    "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1.\n",
    "\n",
    "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and\n",
    "then dividing by the normalization constant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6331,  0.8795, -0.6842,  0.4533,  0.2912])\n",
      "tensor([ 0.0835,  0.3791,  0.0794,  0.2475,  0.2105])\n",
      "tensor(1.)\n",
      "tensor([-2.4825, -0.9700, -2.5337, -1.3962, -1.5583])\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions\n",
    "\n",
    "The objective function is the function that your network is being trained to minimize (in which case it is often called a *loss function*\n",
    "or *cost function*). \n",
    "\n",
    "This proceeds by first choosing a training instance, running it through your neural network, and then computing the loss of the output. \n",
    "\n",
    "The parameters of the model are then updated by taking the derivative of the loss function. \n",
    "\n",
    "Intuitively, if your model is completely confident in its answer, and its answer is wrong, your loss will be high. If it is very confident in its answer, and its answer is correct, the loss will be low.\n",
    "\n",
    "The idea behind minimizing the loss function on your training examples is that your network will hopefully generalize well and have small loss on unseen examples in your dev set, test set, or in production. \n",
    "\n",
    "An example loss function is the *negative log likelihood loss*, which is a very common objective for multi-class classification. \n",
    "\n",
    "For supervised multi-class classification, this means training the network to minimize the negative log probability of the correct output (or equivalently,maximize the log probability of the correct output).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and Training\n",
    "=========================\n",
    "\n",
    "So what we can compute a loss function for an instance? What do we do\n",
    "with that? We saw earlier that Tensors know how to compute gradients\n",
    "with respect to the things that were used to compute it. Well,\n",
    "since our loss is an Tensor, we can compute gradients with\n",
    "respect to all of the parameters used to compute it! Then we can perform\n",
    "standard gradient updates. Let $\\theta$ be our parameters,\n",
    "$L(\\theta)$ the loss function, and $\\eta$ a positive\n",
    "learning rate. Then:\n",
    "\n",
    "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
    "\n",
    "There are a huge collection of algorithms and active research in\n",
    "attempting to do something more than just this vanilla gradient update.\n",
    "Many attempt to vary the learning rate based on what is happening at\n",
    "train time. You don't need to worry about what specifically these\n",
    "algorithms are doing unless you are really interested. Torch provides\n",
    "many in the torch.optim package, and they are all completely\n",
    "transparent. Using the simplest gradient update is the same as the more\n",
    "complicated algorithms. Trying different update algorithms and different\n",
    "parameters for the update algorithms (like different initial learning\n",
    "rates) is important in optimizing your network's performance. Often,\n",
    "just replacing vanilla SGD with an optimizer like Adam or RMSProp will\n",
    "boost performance noticably.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Network Components in PyTorch\n",
    "======================================\n",
    "\n",
    "Before we move on to our focus on NLP, lets do an annotated example of\n",
    "building a network in PyTorch using only affine maps and\n",
    "non-linearities. We will also see how to compute a loss function, using\n",
    "PyTorch's built in negative log likelihood, and update parameters by\n",
    "backpropagation.\n",
    "\n",
    "All network components should inherit from nn.Module and override the\n",
    "forward() method. That is about it, as far as the boilerplate is\n",
    "concerned. Inheriting from nn.Module provides functionality to your\n",
    "component. For example, it makes it keep track of its trainable\n",
    "parameters, you can swap it between CPU and GPU with the ``.to(device)``\n",
    "method, where device can be a CPU device ``torch.device(\"cpu\")`` or CUDA\n",
    "device ``torch.device(\"cuda:0\")``.\n",
    "\n",
    "Let's write an annotated example of a network that takes in a sparse\n",
    "bag-of-words representation and outputs a probability distribution over\n",
    "two labels: \"English\" and \"Spanish\". This model is just logistic\n",
    "regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Logistic Regression Bag-of-Words classifier\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Our model will map a sparse BoW representation to log probabilities over\n",
    "labels. We assign each word in the vocab an index. For example, say our\n",
    "entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
    "respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
    "is\n",
    "\n",
    "\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n",
    "\n",
    "For \"hello world world hello\", it is\n",
    "\n",
    "\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n",
    "\n",
    "etc. In general, it is\n",
    "\n",
    "\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n",
    "\n",
    "Denote this BOW vector as $x$. The output of our network is:\n",
    "\n",
    "\\begin{align}\\log \\text{Softmax}(Ax + b)\\end{align}\n",
    "\n",
    "That is, we pass the input through an affine map and then do log\n",
    "softmax.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "Parameter containing:\n",
      "tensor([[ 0.0669,  0.0351, -0.0834, -0.0594,  0.1796, -0.0363,  0.1106,\n",
      "          0.0849, -0.1268, -0.1668,  0.1882,  0.0102,  0.1344,  0.0406,\n",
      "          0.0631,  0.1465,  0.1860, -0.1301,  0.0245,  0.1464,  0.1421,\n",
      "          0.1218, -0.1419, -0.1412, -0.1186,  0.0246],\n",
      "        [ 0.1955, -0.1239,  0.1045, -0.1085, -0.1844, -0.0417,  0.1130,\n",
      "          0.1821, -0.1218,  0.0426,  0.1692,  0.1300,  0.1222,  0.1394,\n",
      "          0.1240,  0.0507, -0.1341, -0.1647, -0.0899, -0.0228, -0.1202,\n",
      "          0.0717,  0.0607, -0.0444,  0.0754,  0.0634]])\n",
      "Parameter containing:\n",
      "tensor([ 0.1197,  0.1321])\n",
      "tensor([[-0.5765, -0.8252]])\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = data[0]\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the above values corresponds to the log probability of ENGLISH,\n",
    "and which to SPANISH? We never defined it, but we need to if we want to\n",
    "train the thing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets train! To do this, we pass instances through to get log\n",
    "probabilities, compute a loss function, compute the gradient of the loss\n",
    "function, and then update the parameters with a gradient step. Loss\n",
    "functions are provided by Torch in the nn package. nn.NLLLoss() is the\n",
    "negative log likelihood loss we want. It also defines optimization\n",
    "functions in torch.optim. Here, we will just use SGD.\n",
    "\n",
    "Note that the *input* to NLLLoss is a vector of log probabilities, and a\n",
    "target label. It doesn't compute the log probabilities for us. This is\n",
    "why the last layer of our network is log softmax. The loss function\n",
    "nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log\n",
    "softmax for you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9153, -0.5115]])\n",
      "tensor([[-0.6476, -0.7409]])\n",
      "tensor([ 0.1882,  0.1692])\n",
      "tensor([[-0.1673, -1.8703]])\n",
      "tensor([[-2.6570, -0.0727]])\n",
      "tensor([ 0.6609, -0.3034])\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the right answer! You can see that the log probability for\n",
    "Spanish is much higher in the first example, and the log probability for\n",
    "English is much higher in the second for the test data, as it should be.\n",
    "\n",
    "Now you see how to make a PyTorch component, pass some data through it\n",
    "and do gradient updates. We are ready to dig deeper into what deep NLP\n",
    "has to offer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
