{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks 神经网络\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "神经网络可以用``torch.nn``包来构建。\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on ``autograd`` to define models and differentiate them.\n",
    "\n",
    "你已经大致了解了``autograd``， ``nn``依赖于 ``autograd`` 去定义模型并对其进行区分。\n",
    "\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)`` that returns the ``output``.\n",
    "\n",
    "``nn.Module`` 包含层，方法``forward(input)``返回``output``\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "例如，下面这个网络可以给数字图片分类：\n",
    "\n",
    ".. figure:: /_static/img/mnist.png\n",
    "   :alt: convnet\n",
    "\n",
    "   convnet\n",
    "\n",
    "It is a simple feed-forward network. \n",
    "\n",
    "这是一个简单的前向传播网络。\n",
    "\n",
    "It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "获取输入，将它通过几个层一个接一个地传递，然后最终给出输出。\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "典型的神经网络训练过程如下：\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- 定义具有可学习参数（或权重）的神经网络\n",
    "- Iterate over a dataset of inputs\n",
    "- 迭代一个输入的数据集\n",
    "- Process input through the network\n",
    "- 通过神经网络处理输入值\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- 计算损失（输出值和正确值之间的差距）\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- 将梯度反向传播给网络的参数\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "- 更新网络的权重，通常使用一个简单的更新规则：\n",
    "\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "  \n",
    "  权重 = 权重 - 学习速率 * 梯度\n",
    "\n",
    "Define the network 定义网络\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n",
    "让我们来定义这个网络:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsampling是二次取样，在深度神经网络里面和pooling是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积→子采样→卷积→子采样→全连接→全连接→高斯连接测试\n",
    "\n",
    "https://blog.csdn.net/songyimin1208/article/details/68952210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self): # 构造函数\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        # 1个输入图像通道，6个输出通道，5x5平方的卷积核  \n",
    "        #建立两个2维卷积层\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        #三个全连接层\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x): #注意，2D卷积层的输入data维数是 batchsize*channel*height*width\n",
    "        # Max pooling over a (2, 2) window \n",
    "        # 通过（2，2）窗口最大池化\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        # 如果大小是正方形你只能指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension 除了batch维度之外的所有维度\n",
    "        num_features = 1 \n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward`` function (where gradients are computed) is automatically defined for you using ``autograd``.\n",
    "\n",
    "你必须定义“forward”函数，使用“autograd”时“backward”函数（计算梯度）会自动定义\n",
    "\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "在\"forward\"函数中，你可以使用所有的Tensor操作。\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n",
    "模型的学习参数``net.parameters()``由返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try a random 32x32 input\n",
    "\n",
    "我们尝试输入一个32 x 32矩阵\n",
    "\n",
    "Note: Expected input size to this net(LeNet) is 32x32. \n",
    "\n",
    "注意：这个网络的预期输入大小（LeNet）是32x32。\n",
    "\n",
    "To use this net on MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n",
    "要在MNIST数据集上使用这个网络，请将数据从数据集调整到32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0412,  0.0261, -0.0126,  0.1076, -0.0326, -0.0807, -0.0452,\n",
      "         -0.0570, -0.0551, -0.1073]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "\n",
    "梯度缓存参数全部置零，反向传播随机梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap扼要重述:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered在这一点上，我们谈到：**\n",
    "  -  Defining a neural network 定义一个神经网络\n",
    "  -  Processing inputs and calling backward 处理输入和向后调用\n",
    "\n",
    "**Still Left 还剩下:**\n",
    "  -  Computing the loss 计算损失\n",
    "  -  Updating the weights of the network 更新网络的权重\n",
    "\n",
    "Loss Function 损失函数\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "一个损失函数获取一对输入（输出，目标），计算一个值来估计输出离目标有多远。\n",
    "\n",
    "There are several different`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the nn package .\n",
    "\n",
    "nn包里面有几个不同的损失函数。\n",
    "\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error between the input and the target.\n",
    "\n",
    "计算均方误差\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.0210)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1, 11)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its``.grad_fn`` attribute, you will see a graph of computations that looks like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has ``requres_grad=True`` will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x00000214D9C90A90>\n",
      "<AddmmBackward object at 0x00000214DA5F9748>\n",
      "<ExpandBackward object at 0x00000214D9C90A90>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop 反向传播\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0153,  0.0579, -0.1072, -0.0156,  0.0183,  0.0125])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights 更新权值\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "在实践中使用的最简单的更新规则是随机梯度下降（SGD）：\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    "我们可以使用简单的python代码来实现这一点：\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "\n",
    "然而，当您使用神经网络时，您希望使用各种不同的更新规则，如SGD、Nesterov-SGD、Adam、rms道具等。\n",
    "\n",
    "To enable this, we built a small package: ``torch.optim`` that implements all these methods. Using it is very simple:\n",
    "\n",
    "为了实现这一目的，我们建造了一个小包：``torch.optim``实现了所有这些方法。使用它非常简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in `Backprop`_ section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
