{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 《自然语言处理入门》2.9 准确率评测\n",
    "import re\n",
    "import jieba\n",
    "import hanlp\n",
    "import time\n",
    "import thulac\n",
    "from snownlp import SnowNLP\n",
    "from pyhanlp import *\n",
    "from test_utility import ensure_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_region(segmentation: str) -> list:\n",
    "    \"\"\"\n",
    "    将分词结果转换为区间\n",
    "    :param segmentation: 商品 和 服务\n",
    "    :return: [(0, 2), (2, 3), (3, 5)]\n",
    "    \"\"\"\n",
    "    region = []\n",
    "    start = 0\n",
    "    for word in re.compile(\"\\\\s+\").split(segmentation.strip()):\n",
    "        end = start + len(word)\n",
    "        region.append((start, end))\n",
    "        start = end\n",
    "    return region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prf(gold: str, pred: str) -> tuple:\n",
    "    \"\"\"\n",
    "    计算P、R、F1\n",
    "    : gold: 标准答案文件，比如“商品 和 服务”\n",
    "    : pred: 分词结果文件，比如“商品 和服 务”\n",
    "    \"\"\"\n",
    "    A_size, B_size, A_cap_B_size = 0, 0, 0\n",
    "    with open(gold, encoding='utf-8') as gd, open(pred, encoding='utf-8') as pd:\n",
    "        for g, p in zip(gd, pd):\n",
    "            A, B = set(to_region(g)), set(to_region(p))\n",
    "            A_size += len(A)\n",
    "            B_size += len(B)\n",
    "            A_cap_B_size += len(A & B)\n",
    "            text = re.sub(\"\\\\s+\", \"\", g)\n",
    "            \n",
    "    p, r = A_cap_B_size / B_size * 100, A_cap_B_size / A_size * 100\n",
    "    return p, r, 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighan05 = ensure_data('icwb2-data', 'http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用msr语料库 ，另外还有'cityu', 'as', 'msr', 'pku'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\anaconda3\\lib\\site-packages\\pyhanlp\\static\\data\\test\\icwb2-data\\gold\\msr_training_words.utf8\n",
      "C:\\Users\\David\\anaconda3\\lib\\site-packages\\pyhanlp\\static\\data\\test\\icwb2-data\\testing\\msr_test.utf8\n",
      "C:\\Users\\David\\anaconda3\\lib\\site-packages\\pyhanlp\\static\\data\\test\\icwb2-data\\testing\\msr_output.txt\n",
      "C:\\Users\\David\\anaconda3\\lib\\site-packages\\pyhanlp\\static\\data\\test\\icwb2-data\\gold\\msr_test_gold.utf8\n"
     ]
    }
   ],
   "source": [
    "msr_dict = os.path.join(sighan05, 'gold', 'msr_training_words.utf8')\n",
    "msr_test = os.path.join(sighan05, 'testing', 'msr_test.utf8')\n",
    "msr_output = os.path.join(sighan05, 'testing', 'msr_output.txt')\n",
    "msr_gold = os.path.join(sighan05, 'gold', 'msr_test_gold.utf8')\n",
    "\n",
    "print(msr_dict)\n",
    "print(msr_test)\n",
    "print(msr_output)\n",
    "print(msr_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "    def jieba_segment(self, sentence):\n",
    "        seg_list = jieba.cut(sentence, cut_all=False)\n",
    "        sentence = ' '.join(seg_list)\n",
    "        return sentence\n",
    "\n",
    "    def hanlp_segment(self, han_tokenizer, sentence):\n",
    "        # hanlp分词\n",
    "        sentence = han_tokenizer(sentence)\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def thulac_segment(self, thu, sentence):\n",
    "        # thulac 分词\n",
    "        sentence = thu.cut(sentence, text=True)  # 进行一句话分词\n",
    "        # cut_f(输入文件, 输出文件)\n",
    "        return ''.join(sentence) # 注意：此处单引号不要空格，否则结果不对。 David 2020.8.13\n",
    "\n",
    "    def pkuseg_segment(self, sentence):\n",
    "        seg = pkuseg.pkuseg(postag=False)  # 以默认配置加载模型\n",
    "        sentence = seg.cut(sentence)  # 进行分词\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def pynlpir_segment(self, sentence):\n",
    "        # pynlpir分词\n",
    "        pynlpir.open()\n",
    "        sentence = pynlpir.segment(sentence, pos_tagging=False)\n",
    "        pynlpir.close()\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def snownlp_segment(self, sentence):\n",
    "        # snownlp分词\n",
    "        # unicode_sentence = sentence.decode('gbk')\n",
    "        sentence = SnowNLP(sentence).words\n",
    "        return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入要使用的 中文分词 包：'hanlp', 'jieba', 'snownlp', 'pynlpir', 'pkuseg', 'thulac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thulac_segment\n"
     ]
    }
   ],
   "source": [
    "# 'hanlp', 'jieba', 'snownlp', 'pynlpir', 'pkuseg', 'thulac'\n",
    "segment_func = 'thulac'+'_segment'\n",
    "print(segment_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "if 'thulac' in segment_func:\n",
    "    thu = thulac.thulac(seg_only=True, deli=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "with open(msr_gold, encoding='utf-8') as test, open(msr_output, 'w', encoding='utf-8') as output:\n",
    "    for line in test:\n",
    "#         print(line)\n",
    "        line = getattr(Segment(), segment_func)(thu, re.sub(\"\\\\s+\", \"\", line))  # 根据参数调用不同分词工具\n",
    "#         print(line)\n",
    "#         print('\\n')\n",
    "        output.write(line)\n",
    "        output.write(\"\\n\")   \n",
    "end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词运行时间： 4.750853299999999\n",
      "P:83.38 \n",
      "R:87.86 \n",
      "F1:85.56 \n"
     ]
    }
   ],
   "source": [
    "print('分词运行时间：', end - start)\n",
    "print(\"P:%.2f \\nR:%.2f \\nF1:%.2f \" % prf(msr_gold, msr_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
