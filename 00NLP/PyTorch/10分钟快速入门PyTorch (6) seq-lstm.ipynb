{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "training_data = [(\"The dog ate the apple\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\",\"NN\"])]\n",
    "\n",
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "for context, tag in training_data:\n",
    "    for word in context:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for label in tag:\n",
    "        if label not in tag_to_idx:\n",
    "            tag_to_idx[label] = len(tag_to_idx)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "character_to_idx = {}\n",
    "for i in range(len(alphabet)):\n",
    "    character_to_idx[alphabet[i]] = i\n",
    "\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_char, char_dim, char_hidden):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.char_embedding = nn.Embedding(n_char, char_dim)\n",
    "        self.char_lstm = nn.LSTM(char_dim, char_hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.char_embedding(x)\n",
    "        _, h = self.char_lstm(x)\n",
    "        return h[0]\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden, n_hidden,\n",
    "                 n_tag):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(n_word, n_dim)\n",
    "        self.char_lstm = CharLSTM(n_char, char_dim, char_hidden)\n",
    "        self.lstm = nn.LSTM(n_dim + char_hidden, n_hidden, batch_first=True)\n",
    "        self.linear1 = nn.Linear(n_hidden, n_tag)\n",
    "\n",
    "    def forward(self, x, word):\n",
    "        char = torch.FloatTensor()\n",
    "        for each in word:\n",
    "            char_list = []\n",
    "            for letter in each:\n",
    "                char_list.append(character_to_idx[letter.lower()])\n",
    "            char_list = torch.LongTensor(char_list)\n",
    "            char_list = char_list.unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                tempchar = self.char_lstm(Variable(char_list).cuda())\n",
    "            else:\n",
    "                tempchar = self.char_lstm(Variable(char_list))\n",
    "            tempchar = tempchar.squeeze(0)\n",
    "            char = torch.cat((char, tempchar.cpu().data), 0)\n",
    "        if torch.cuda.is_available():\n",
    "            char = char.cuda()\n",
    "        char = Variable(char)\n",
    "        x = self.word_embedding(x)\n",
    "        x = torch.cat((x, char), 1)\n",
    "        x = x.unsqueeze(0)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.linear1(x)\n",
    "        y = F.log_softmax(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = LSTMTagger(\n",
    "    len(word_to_idx), len(character_to_idx), 10, 100, 50, 128, len(tag_to_idx))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def make_sequence(x, dic):\n",
    "    idx = [dic[i] for i in x]\n",
    "    idx = Variable(torch.LongTensor(idx))\n",
    "    return idx\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    running_loss = 0\n",
    "    for data in training_data:\n",
    "        word, tag = data\n",
    "        word_list = make_sequence(word, word_to_idx)\n",
    "        tag = make_sequence(tag, tag_to_idx)\n",
    "        if torch.cuda.is_available():\n",
    "            word_list = word_list.cuda()\n",
    "            tag = tag.cuda()\n",
    "        # forward\n",
    "        out = model(word_list, word)\n",
    "        loss = criterion(out, tag)\n",
    "        running_loss += loss.data[0]\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {}'.format(running_loss / len(data)))\n",
    "print()\n",
    "input = make_sequence(\"Everybody ate the apple\".split(), word_to_idx)\n",
    "if torch.cuda.is_available():\n",
    "    input = input.cuda()\n",
    "\n",
    "out = model(input, \"Everybody ate the apple\".split())\n",
    "print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
